<script lang="ts">
  import { onMount, onDestroy, tick } from "svelte";
  import { modelsAPI } from "../../../lib/api/models";

  import InferenceAPI, {
    type InferencePrompt,
    type InferenceConfig,
  } from "../../../lib/api/inference";

  import {
    drawInferenceResults,
    drawInferenceResultsScaled,
    drawInferenceResultsToDataURL,
    drawPolygonMask,
    drawBoundingBox,
    type DrawOptions,
  } from "../../../lib/utils/drawingUtils";

  import {
    processInferenceResponse,
    normalizeInferenceResponse,
    type ProcessedResults,
    type FrameStats,
  } from "../../../lib/utils/responseProcessor";

  import { campaignsAPI } from "../../../lib/api/campaigns";
  import { playbooksAPI } from "../../../lib/api/playbooks";
  import type {
    Model,
    PredictionResponse,
    PredictionResponseWithFrame,
    PredictionJob,
    MaskData,
    PredictionResult,
  } from "../../../types";

  import type { Campaign } from "../../../lib/types/campaign";
  import { uiStore } from "../../../lib/stores/uiStore";
  import RTSPViewer from "../../../lib/components/visionmask/RTSPViewer.svelte";

  // Feature Flags and New Components (Phase 1)
  import {
    useNewComponents,
    useNewModes,
  } from "../../../lib/utils/featureFlags";
  import SourceTypeSelector from "../../../lib/components/capture/SourceTypeSelector.svelte";
  import ModelSelector from "../../../lib/components/capture/ModelSelector.svelte";
  import SmartSettingsPanel from "../../../lib/components/capture/SmartSettingsPanel.svelte";
  import StatsPanel from "../../../lib/components/capture/StatsPanel.svelte";
  import CaptureGallery from "../../../lib/components/capture/CaptureGallery.svelte";
  import FileUploadControl from "../../../lib/components/capture/FileUploadControl.svelte";
  import CaptureControls from "../../../lib/components/capture/CaptureControls.svelte";

  // Phase 2 Components
  import MediaDisplay from "../../../lib/components/capture/MediaDisplay.svelte";
  import ViewModeSwitcher from "../../../lib/components/capture/ViewModeSwitcher.svelte";

  import { featureFlags } from "../../../lib/utils/featureFlags";

  // // Later in your code
  // featureFlags.update(($flags) => ({
  //   ...$flags,
  //   useNewComponents: true,
  //   useNewModes: true,
  // }));

  // Optional session context
  export let campaignId: number | undefined = undefined;
  export let playbookId: number | undefined = undefined;
  let campaign: Campaign | null = null;

  // State
  let models: Model[] = [];
  let selectedModelId: number | null = null;
  let selectedModel: Model | null = null;
  let confidence = 0.5;
  let skipFrames = 5; // Frame skip for video/RTSP (1=all frames, 5=every 5th frame)
  let sourceType: "image" | "batch" | "video" | "webcam" | "rtsp" = "image";

  // Media elements
  let videoElement: HTMLVideoElement;
  let canvasElement: HTMLCanvasElement;
  let canvasOverlay: HTMLCanvasElement;
  let rtspCanvasElement: HTMLCanvasElement;
  let rtspCanvasOverlay: HTMLCanvasElement;
  let fileInputElement: HTMLInputElement;
  let imagePreview: string | null = null;
  let selectedFile: File | null = null;
  let selectedFiles: File[] = [];
  let rtspUrl = "";

  // Detection state
  let isDetecting = false;
  let detections: PredictionResponse | null = null;
  let currentJob: PredictionJob | null = null;
  let detectionResults: Array<{ class_name: string; confidence: number }> = [];
  let frameStats: FrameStats = {
    width: 0,
    height: 0,
    fps: 0,
    totalDetections: 0,
    totalMasks: 0,
    avgConfidence: 0,
    task_type: "",
  };
  let classCounts: Record<string, number> = {};

  // Model That Requires Prompts
  let promptRequired = false;
  let inferPrompts: InferencePrompt[] = [];
  let promptMode: "auto" | "text" | "point" | "box" = "auto";
  let textPrompt = "";
  let isDrawingBox = false;
  let boxStartCoords: { x: number; y: number } | null = null;
  let tempBoxCoords: { x1: number; y1: number; x2: number; y2: number } | null =
    null;
  let batchProcessingProgress = { current: 0, total: 0, fileName: "" };

  // Gallery state
  let galleryImages: Array<{
    original: string;
    annotated: string;
    fileName: string;
    timestamp?: number; // For video frames
    detectionData?: PredictionResponse;
  }> = [];
  let currentGalleryIndex = 0;
  let selectedImageIndex = 0; // For webcam gallery view

  // Webcam view mode state (iPhone-style switcher)
  let webcamViewMode: "live" | "gallery" = "live";

  // Webcam capture mode (manual vs continuous)
  let webcamCaptureMode: "continuous" | "manual" = "manual";
  let lastPredictionResponse: PredictionResponse | null = null;
  let isFlashing = false;

  // RTSP view mode and capture mode
  let rtspViewMode: "live" | "gallery" = "live";
  let rtspCaptureMode: "continuous" | "manual" = "manual";
  let rtspLastFrameData: {
    frame: string;
    predictions: PredictionResponse;
  } | null = null;
  let processedFrameNumbers = new Set<number>();
  let rtspFrameStatus: "loading" | "ready" | "error" = "loading";

  // RTSP viewer component reference
  let rtsp_viewer: RTSPViewer | null = null;

  // Video capture mode (manual vs continuous)
  let videoCaptureMode: "manual" | "continuous" = "manual";
  let videoDetectionIntervalId: number | null = null;

  // Video session state (for manual mode)
  let activeVideoSession: PredictionJob | null = null;
  let videoSessionHeartbeatInterval: number | null = null;
  let showInactivityModal = false;
  let inactivityWarningTimeout: number | null = null;

  // Webcam session state (for manual mode)
  let activeWebcamSession: PredictionJob | null = null;

  // Reactive statement to render RTSP polygon masks when frame data updates
  $: if (
    rtspLastFrameData &&
    rtspCanvasElement &&
    rtspCanvasOverlay &&
    sourceType === "rtsp" &&
    rtspViewMode === "live"
  ) {
    drawRTSPLiveFrame(rtspLastFrameData);
  }

  // Reactive statement to update current frame stats when gallery index changes
  // RS2 FIX: Cancel pending overlay updates to prevent desync
  $: if (
    galleryImages.length > 0 &&
    galleryImages[currentGalleryIndex]?.detectionData
  ) {
    const data = galleryImages[currentGalleryIndex].detectionData!;
    updateCurrentFrameStats(data);
    // If video mode and timestamp exists, seek to that position and update overlay
    if (
      sourceType === "video" &&
      galleryImages[currentGalleryIndex].timestamp !== undefined &&
      videoElement
    ) {
      // Cancel pending overlay update
      if (overlayUpdateTimer) {
        clearTimeout(overlayUpdateTimer);
        overlayUpdateTimer = null;
      }

      videoElement.currentTime = galleryImages[currentGalleryIndex].timestamp!;
      // Schedule new overlay update
      if (videoCaptureMode === "continuous") {
        overlayUpdateTimer = window.setTimeout(() => {
          updateVideoOverlay();
          overlayUpdateTimer = null;
        }, 100);
      }
    }
  }

  // Reactive statement to handle webcam access when source type changes
  // RS3 FIX: Use AbortController to prevent stream orphaning on rapid mode switches
  $: if (sourceType === "webcam") {
    // Start webcam preview when user selects webcam source (before detection)
    if (!webcamStream && !isDetecting) {
      // Cancel previous request if still pending
      if (webcamAbortController) {
        webcamAbortController.abort();
      }
      webcamAbortController = new AbortController();
      const currentController = webcamAbortController;

      (async () => {
        try {
          const stream = await navigator.mediaDevices.getUserMedia({
            video: { width: 640, height: 480 },
          });

          // Check if this request was aborted
          if (currentController.signal.aborted) {
            stream.getTracks().forEach((track) => track.stop());
            return;
          }

          webcamStream = stream;
          if (videoElement) {
            videoElement.srcObject = webcamStream;
            videoElement.play();
            frameStats.width = 640;
            frameStats.height = 480;
          }
        } catch (error: any) {
          if (error.name === "AbortError") return;
          console.error("Failed to access webcam:", error);
          uiStore.showError(
            "Failed to access webcam. Please check permissions.",
            "Webcam Access Error",
          );
        }
      })();
    }
  } else {
    // Stop webcam when user switches away from webcam source
    if (webcamAbortController) {
      webcamAbortController.abort();
      webcamAbortController = null;
    }
    if (webcamStream) {
      webcamStream.getTracks().forEach((track) => track.stop());
      webcamStream = null;
      if (videoElement) {
        videoElement.srcObject = null;
      }
    }
  }

  // Auto-switch prompt mode to "text" for RTSP (since point/box are disabled)
  $: if (promptRequired && sourceType === "rtsp") {
    if (promptMode !== "text") {
      promptMode = "text";
    }
  }

  // Auto-save Smart Settings when changed
  // RS5 FIX: Debounce to prevent localStorage spam during slider drag
  $: if (
    selectedModelId &&
    (confidence || classFilter || skipFrames !== undefined)
  ) {
    if (settingsSaveTimer) {
      clearTimeout(settingsSaveTimer);
    }
    settingsSaveTimer = window.setTimeout(() => {
      saveSmartSettings();
      settingsSaveTimer = null;
    }, 300);
  }

  // Auto-save prompt Settings when changed
  $: if (selectedModelId && promptRequired && (inferPrompts || promptMode)) {
    savePrompts();
  }

  // RS7+RS8 FIX: Canvas draw queue to prevent overlapping draws
  async function processCanvasQueue() {
    if (canvasDrawing || canvasDrawQueue.length === 0) return;
    canvasDrawing = true;

    while (canvasDrawQueue.length > 0) {
      const draw = canvasDrawQueue.shift()!;
      draw();
      await tick(); // Allow DOM to update
    }

    canvasDrawing = false;
  }

  function queueCanvasDraw(drawFn: () => void) {
    canvasDrawQueue.push(drawFn);
    processCanvasQueue();
  }

  // Draw model prompts when image preview or prompts change (for models requiring prompts)
  // RS7: Queue image load and initial draw
  $: if (
    imagePreview &&
    canvasElement &&
    promptRequired &&
    sourceType === "image"
  ) {
    queueCanvasDraw(() => {
      const ctx = canvasElement?.getContext("2d");
      if (ctx && imagePreview) {
        const img = new Image();
        img.onload = () => {
          canvasElement!.width = img.width;
          canvasElement!.height = img.height;
          ctx.drawImage(img, 0, 0);
          drawInferencePrompts();
        };
        img.src = imagePreview;
      }
    });
  }

  // Redraw prompts component when prompts, temp box, or drawing state changes
  // RS8: Queue prompt overlay redraw with RAF throttling
  let promptDrawPending = false;
  $: if (
    canvasElement &&
    promptRequired &&
    imagePreview &&
    (inferPrompts || tempBoxCoords || isDrawingBox)
  ) {
    if (!promptDrawPending) {
      promptDrawPending = true;
      requestAnimationFrame(() => {
        queueCanvasDraw(() => drawInferencePrompts());
        promptDrawPending = false;
      });
    }
  }

  // Filters
  let selectedClasses: Set<string> = new Set();
  let availableClasses: string[] = [];
  let classFilter: string[] = []; // Pre-detection class filter
  let newClassTag = ""; // Input for adding new class filter tags

  // UI state
  let isTipsCollapsed = false; // Collapsible tips section

  // Zoom state
  let zoomLevel = 1;
  let isPanning = false;
  let panStart = { x: 0, y: 0 };
  let panOffset = { x: 0, y: 0 };

  // Animation & polling
  let animationFrameId: number;
  let pollingIntervalId: number;
  let webcamStream: MediaStream | null = null;

  // Reactive statement fix variables
  let webcamAbortController: AbortController | null = null; // RS3 fix
  let overlayUpdateTimer: number | null = null; // RS2 fix
  let settingsSaveTimer: number | null = null; // RS5 fix
  let canvasDrawQueue: Array<() => void> = []; // RS7+RS8 fix
  let canvasDrawing = false; // RS7+RS8 fix

  onMount(() => {
    // Load data asynchronously (fire and forget)
    (async () => {
      // Load campaign first if campaignId is provided
      if (campaignId) {
        await loadCampaign();
      }
      await loadModels();
    })();

    // Add keyboard listener for capture shortcut
    const handleKeyPress = (e: KeyboardEvent) => {
      // For video manual capture mode, prevent space bar from controlling video playback
      if (
        e.code === "Space" &&
        isDetecting &&
        sourceType === "video" &&
        videoCaptureMode === "manual"
      ) {
        e.preventDefault();
        e.stopPropagation();
        if (activeVideoSession && videoElement) {
          captureCurrentVideoFrame();
        }
        return;
      }

      if ((e.code === "Space" || e.code === "Enter") && isDetecting) {
        if (
          sourceType === "webcam" &&
          webcamCaptureMode === "manual" &&
          lastPredictionResponse
        ) {
          e.preventDefault();
          captureCurrentCameraFrame();
        } else if (
          sourceType === "rtsp" &&
          rtspCaptureMode === "manual" &&
          rtspLastFrameData
        ) {
          e.preventDefault();
          captureCurrentRTSPFrame();
        }
      }
    };

    window.addEventListener("keydown", handleKeyPress);

    return () => {
      window.removeEventListener("keydown", handleKeyPress);
    };
  });

  onDestroy(() => {
    cleanup();
  });

  async function loadCampaign() {
    if (!campaignId) return;

    try {
      campaign = await campaignsAPI.getCampaign(campaignId);
    } catch (error) {
      console.error("Failed to load campaign:", error);
      uiStore.showError(
        "Failed to load campaign details. Please try again later.",
        "Campaign Load Error",
      );
    }
  }

  async function loadModels() {
    try {
      let availableModels: any[] = [];

      if (playbookId) {
        // If playbookId is provided, load only models from that playbook
        const playbookModels = await playbooksAPI.getModels(playbookId);
        const playbookModelIds = playbookModels.map((pm) => pm.model_id);

        // Load all ready models and filter by playbook
        const allModels = await modelsAPI.list(0, 100);
        availableModels = allModels.filter(
          (m) => m.status === "ready" && playbookModelIds.includes(m.id),
        );
      } else {
        // Load all ready models (no playbook filter)
        const allModels = await modelsAPI.list(0, 100);
        availableModels = allModels.filter((m) => m.status === "ready");
      }

      models = availableModels;
    } catch (error) {
      console.error("Failed to load models:", error);
      uiStore.showError(
        "Failed to load models. Please try again later.",
        "Model Load Error",
      );
    }
  }

  function handleModelSelect(event: Event) {
    const target = event.target as HTMLSelectElement;
    selectedModelId = target.value ? parseInt(target.value) : null;
    selectedModel = models.find((m) => m.id === selectedModelId) || null;

    // Detect model that requires prompts
    // Backward compatibility: Check requires_prompts first, fallback to no prompts inference types
    promptRequired = selectedModel?.requires_prompts || false;

    // Reset prompt state when model changes (but keep saved prompts from settings)
    if (promptRequired) {
      loadPromptsSettings();
      // Force text mode for RTSP (point/box not supported)
      if (sourceType === "rtsp") {
        promptMode = "text";
      }
    } else {
      inferPrompts = [];
      promptMode = "auto";
    }

    // Load Smart Settings for this model (including confidence)
    loadSmartSettings();
  }

  // Event handler adapters for new components (Phase 1)
  function handleComponentModelChange(detail: {
    modelId: number;
    model: Model | null;
  }) {
    selectedModelId = detail.modelId;
    selectedModel = detail.model;

    // Detect model that requires prompts
    promptRequired = selectedModel?.requires_prompts || false;

    // Reset prompt state when model changes (but keep saved prompts from settings)
    if (promptRequired) {
      loadPromptsSettings();
      // Force text mode for RTSP (point/box not supported)
      if (sourceType === "rtsp") {
        promptMode = "text";
      }
    } else {
      inferPrompts = [];
      promptMode = "auto";
    }

    // Load Smart Settings for this model (including confidence)
    loadSmartSettings();
  }

  function handleComponentFileSelect(file: File) {
    selectedFile = file;
    if (sourceType === "image" && file.type.startsWith("image/")) {
      const reader = new FileReader();
      reader.onload = (e) => {
        imagePreview = e.target?.result as string;
        // Update frame stats
        const img = new Image();
        img.onload = () => {
          frameStats.width = img.width;
          frameStats.height = img.height;
        };
        img.src = imagePreview;
      };
      reader.readAsDataURL(file);
    } else if (sourceType === "video" && file.type.startsWith("video/")) {
      const url = URL.createObjectURL(file);
      if (videoElement) {
        // Set crossOrigin to prevent canvas tainting
        videoElement.crossOrigin = "anonymous";
        videoElement.src = url;
        videoElement.onloadedmetadata = () => {
          frameStats.width = videoElement.videoWidth;
          frameStats.height = videoElement.videoHeight;
        };
      }
    }
  }

  function handleComponentFilesSelect(files: File[]) {
    selectedFiles = files;
  }

  function handleComponentClearPreview() {
    selectedFile = null;
    selectedFiles = [];
    imagePreview = null;
    galleryImages = [];
    currentGalleryIndex = 0;
  }

  function handleComponentGalleryClear() {
    galleryImages = [];
    selectedImageIndex = 0;
  }

  // Smart Settings persistence
  function loadSmartSettings() {
    if (!selectedModelId) return;

    try {
      const settingsKey = `smart_settings_${selectedModelId}`;
      const stored = localStorage.getItem(settingsKey);
      if (stored) {
        const settings = JSON.parse(stored);

        // Load confidence
        if (typeof settings.confidence === "number") {
          confidence = settings.confidence;
        }

        // Load class filter
        if (Array.isArray(settings.classFilter)) {
          classFilter = settings.classFilter;
        }

        // Load skip frames
        if (typeof settings.skipFrames === "number") {
          skipFrames = settings.skipFrames;
        }
      }
    } catch (error) {
      console.warn("Failed to load smart settings:", error);
    }
  }

  function saveSmartSettings() {
    if (!selectedModelId) return;

    try {
      const settingsKey = `smart_settings_${selectedModelId}`;
      const settings = {
        confidence,
        classFilter,
        skipFrames,
      };
      localStorage.setItem(settingsKey, JSON.stringify(settings));
    } catch (error) {
      console.warn("Failed to save smart settings:", error);
    }
  }

  // Prompts Settings persistence
  function loadPromptsSettings() {
    if (!selectedModelId) return;

    try {
      const settingsKey = `atvision_prompts_model_${selectedModelId}`;
      const stored = localStorage.getItem(settingsKey);
      if (stored) {
        const settings = JSON.parse(stored);

        // Load prompts
        if (Array.isArray(settings.prompts)) {
          inferPrompts = settings.prompts;
        }

        // Load prompt mode
        if (
          settings.promptMode &&
          ["auto", "text", "point", "box"].includes(settings.promptMode)
        ) {
          promptMode = settings.promptMode;
        }
      }
    } catch (error) {
      console.warn("Failed to load Prompts settings:", error);
    }
  }

  function savePrompts() {
    if (!selectedModelId) return;

    try {
      const settingsKey = `atvision_prompts_model_${selectedModelId}`;
      const settings = {
        prompts: inferPrompts,
        promptMode: promptMode,
      };
      localStorage.setItem(settingsKey, JSON.stringify(settings));
    } catch (error) {
      console.warn("Failed to save Prompts settings:", error);
      uiStore.showError(
        "Failed to save Prompts settings. Please try again.",
        "Prompts Settings Error",
      );
    }
  }

  // Prompts Management Functions
  function addtextPrompt() {
    if (!textPrompt.trim()) return;

    inferPrompts = [
      ...inferPrompts,
      {
        type: "text",
        value: textPrompt.trim(),
      },
    ];

    textPrompt = ""; // Clear input
    savePrompts(); // Persist
  }

  function removeInferencePrompt(index: number) {
    inferPrompts = inferPrompts.filter((_, i) => i !== index);
    savePrompts(); // Persist
  }

  function clearInferencePrompts() {
    inferPrompts = [];
    promptMode = "auto";
    savePrompts(); // Persist
  }

  function handleCanvasClick(event: MouseEvent) {
    if (!canvasElement || !promptRequired) return;

    const rect = canvasElement.getBoundingClientRect();
    const canvasWidth = canvasElement.width;
    const canvasHeight = canvasElement.height;
    const rectWidth = rect.width;
    const rectHeight = rect.height;

    // Calculate accurate image coordinates with scaling
    const x = Math.round(
      (event.clientX - rect.left) * (canvasWidth / rectWidth),
    );
    const y = Math.round(
      (event.clientY - rect.top) * (canvasHeight / rectHeight),
    );

    if (promptMode === "point") {
      // Foreground point (green) by default, background (red) with Shift key
      const label = event.shiftKey ? 0 : 1;

      inferPrompts = [
        ...inferPrompts,
        {
          type: "point",
          coords: [x, y],
          label,
        },
      ];

      savePrompts();
    } else if (promptMode === "box") {
      if (!isDrawingBox) {
        // Start drawing box
        isDrawingBox = true;
        boxStartCoords = { x, y };
        tempBoxCoords = { x1: x, y1: y, x2: x, y2: y };
      } else {
        // Finish drawing box
        if (boxStartCoords) {
          const x1 = Math.min(boxStartCoords.x, x);
          const y1 = Math.min(boxStartCoords.y, y);
          const x2 = Math.max(boxStartCoords.x, x);
          const y2 = Math.max(boxStartCoords.y, y);

          inferPrompts = [
            ...inferPrompts,
            {
              type: "box",
              coords: [x1, y1, x2, y2],
            },
          ];

          savePrompts();
        }

        // Reset box drawing state
        isDrawingBox = false;
        boxStartCoords = null;
        tempBoxCoords = null;
      }
    }
  }

  function handleCanvasMouseMove(event: MouseEvent) {
    if (!canvasElement || !promptRequired || !isDrawingBox || !boxStartCoords)
      return;

    const rect = canvasElement.getBoundingClientRect();
    const canvasWidth = canvasElement.width;
    const canvasHeight = canvasElement.height;
    const rectWidth = rect.width;
    const rectHeight = rect.height;

    // Calculate accurate image coordinates with scaling
    const x = Math.round(
      (event.clientX - rect.left) * (canvasWidth / rectWidth),
    );
    const y = Math.round(
      (event.clientY - rect.top) * (canvasHeight / rectHeight),
    );

    // Update temp box preview
    tempBoxCoords = {
      x1: Math.min(boxStartCoords.x, x),
      y1: Math.min(boxStartCoords.y, y),
      x2: Math.max(boxStartCoords.x, x),
      y2: Math.max(boxStartCoords.y, y),
    };
  }

  // Get task-specific upload instructions
  function getTaskInstructions(taskType: string | undefined): string {
    if (!taskType) return "";

    const instructions: Record<string, string> = {
      detect:
        "For object detection, your images should contain the objects you want to detect. The model will identify objects and draw bounding boxes around them.",
      classify:
        "For classification, provide clear images of single subjects. The model will categorize each image into one of the trained classes. Avoid images with multiple subjects or cluttered backgrounds for best results.",
      segment:
        "For segmentation, provide images where you want to identify and outline specific regions or objects. The model will create pixel-level masks for each detected instance.",
    };

    return instructions[taskType] || "";
  }

  // Get task-specific format guidance
  function getTaskFormatGuidance(taskType: string | undefined): {
    title: string;
    items: string[];
  } {
    if (!taskType) return { title: "", items: [] };

    const guidance: Record<string, { title: string; items: string[] }> = {
      detect: {
        title: "Detection Tips:",
        items: [
          "Images can contain multiple objects",
          "Objects should be clearly visible",
          "Various angles and lighting conditions supported",
          "Model will draw bounding boxes around detected objects",
        ],
      },
      classify: {
        title: "Classification Tips:",
        items: [
          "One main subject per image works best",
          "Clear, well-lit images improve accuracy",
          "Avoid cluttered backgrounds when possible",
          "Model will assign a class label and confidence score",
        ],
      },
      segment: {
        title: "Segmentation Tips:",
        items: [
          "Clear object boundaries improve results",
          "Good contrast between objects and background",
          "Model will create pixel-level masks",
          "Can handle multiple instances of same class",
        ],
      },
    };

    return guidance[taskType] || { title: "", items: [] };
  }

  // Check if model task type matches expected use case
  function getTaskMismatchWarning(
    taskType: string | undefined,
    sourceType: string,
  ): string | null {
    if (!taskType) return null;

    // Classification works best with single images, less ideal for batch/video
    if (
      taskType === "classify" &&
      (sourceType === "batch" || sourceType === "video")
    ) {
      return "⚠️ Classification models work best with single images. Batch/video processing may produce less accurate results for classification tasks.";
    }

    return null;
  }

  async function handleFileUpload(event: Event) {
    const target = event.target as HTMLInputElement;
    const files = target.files;
    if (!files || files.length === 0) return;

    if (sourceType === "batch") {
      selectedFiles = Array.from(files);
    } else {
      selectedFile = files[0];

      // Create preview for image
      if (sourceType === "image" && selectedFile.type.startsWith("image/")) {
        const reader = new FileReader();
        reader.onload = (e) => {
          imagePreview = e.target?.result as string;
          // Update frame stats
          const img = new Image();
          img.onload = () => {
            frameStats.width = img.width;
            frameStats.height = img.height;
          };
          img.src = imagePreview;
        };
        reader.readAsDataURL(selectedFile);
      } else if (
        sourceType === "video" &&
        selectedFile.type.startsWith("video/")
      ) {
        const url = URL.createObjectURL(selectedFile);
        if (videoElement) {
          // Set crossOrigin to prevent canvas tainting
          videoElement.crossOrigin = "anonymous";
          videoElement.src = url;
          videoElement.onloadedmetadata = () => {
            frameStats.width = videoElement.videoWidth;
            frameStats.height = videoElement.videoHeight;
          };
        }
      }
    }
  }

  async function startPrediction() {
    if (!selectedModelId) {
      uiStore.showWarning("Please select a model first", "Model Required");
      return;
    }

    isDetecting = true;

    try {
      if (sourceType === "image" && selectedFile) {
        await inferSingleImage();
      } else if (sourceType === "batch" && selectedFiles.length > 0) {
        await inferBatchImage();
      } else if (sourceType === "video" && selectedFile) {
        await inferVideo();
      } else if (sourceType === "webcam") {
        await inferWebCam();
      } else if (sourceType === "rtsp" && rtspUrl) {
        await inferRTSP();
      } else {
        if (sourceType === "rtsp") {
          uiStore.showWarning(
            "Please provide a valid RTSP URL!",
            "Input Required",
          );
        } else {
          uiStore.showWarning(
            "Please select a file to proceed!",
            "Input Required",
          );
        }

        isDetecting = false;
      }
    } catch (error) {
      console.error("Detection error:", error);
      uiStore.showError(
        "Detection failed. Please try again.",
        "Detection Error",
      );
      isDetecting = false;
    }
  }

  async function inferSingleImage() {
    if (!selectedFile || !selectedModelId) return;

    try {
      // Validate Prompts if needed (non-auto modes require prompts)
      if (
        promptRequired &&
        promptMode !== "auto" &&
        inferPrompts.length === 0
      ) {
        uiStore.showWarning(
          `Please add at least one ${promptMode} prompt for Model that requires prompts.`,
          "Prompts Required",
        );
        isDetecting = false;
        return;
      }

      // Build unified inference options
      const options = {
        modelId: selectedModelId,
        confidence: confidence,
        campaignId: campaignId,
        classFilter: classFilter.length > 0 ? classFilter : undefined,
        // Include prompts for model that need prompts (auto mode uses empty array)
        ...(promptRequired && {
          prompts: promptMode === "auto" ? [] : inferPrompts,
        }),
      };

      // Single unified API call for all supported model types
      const response = await InferenceAPI.inferSingle(selectedFile, options);

      // Use response directly - API already provides complete structure
      detections = response;

      processDetectionResults(detections);
      const annotatedImage = await drawDetectionsToDataURL();

      // Add to gallery with detection data
      galleryImages = [
        {
          original: imagePreview!,
          annotated: annotatedImage,
          fileName: selectedFile.name,
          detectionData: detections,
        },
      ];
      currentGalleryIndex = 0;
    } catch (error) {
      console.error("Single image detection failed:", error);
      uiStore.showError(
        "Single image detection failed. Please try again.",
        "Detection Error",
      );
      throw error;
    } finally {
      isDetecting = false;
    }
  }

  async function inferBatchImage() {
    if (selectedFiles.length === 0 || !selectedModelId) return;

    try {
      if (promptRequired) {
        // Validate prompts (except auto mode)
        if (promptMode !== "auto" && inferPrompts.length === 0) {
          uiStore.showWarning(
            `Please add at least one ${promptMode} prompt for Model that requires prompts.`,
            "Prompts Required",
          );
          isDetecting = false;
          return;
        }
      }

      // Now used unified inference API for batch as well.
      currentJob = await InferenceAPI.inferBatch(selectedFiles, {
        modelId: selectedModelId,
        confidence: confidence,
        prompts: promptMode === "auto" ? [] : inferPrompts,
        classFilter: classFilter.length > 0 ? classFilter : undefined,
        campaignId: campaignId,
      });

      // Poll for job completion
      pollJobStatusForBatch();
    } catch (error) {
      console.error("Batch processing failed:", error);
      throw error;
    }
  }

  function pollJobStatusForBatch() {
    pollingIntervalId = window.setInterval(async () => {
      if (!currentJob) return;

      try {
        const job = await InferenceAPI.getJob(currentJob.id);
        currentJob = job;

        if (job.status === "completed" || job.status === "failed") {
          clearInterval(pollingIntervalId);
          isDetecting = false;

          if (job.status === "completed") {
            // Fetch results and build gallery
            const results = await InferenceAPI.getResults(job.id, 0, 100);
            await buildGalleryFromResults(results);
          }
        }
      } catch (error) {
        console.error("Failed to poll job status:", error);
      }
    }, 2000);
  }

  async function buildGalleryFromResults(results: any[]) {
    galleryImages = [];

    for (const result of results) {
      // Create canvas for each result
      const canvas = document.createElement("canvas");
      const ctx = canvas.getContext("2d");
      if (!ctx) continue;

      // Find the original file
      const originalFile = selectedFiles.find(
        (f) => f.name === result.file_name,
      );
      if (!originalFile) continue;

      const img = new Image();
      const imageUrl = await new Promise<string>((resolve) => {
        const reader = new FileReader();
        reader.onload = (e) => resolve(e.target?.result as string);
        reader.readAsDataURL(originalFile);
      });

      await new Promise<void>((resolve) => {
        img.onload = async () => {
          canvas.width = img.width;
          canvas.height = img.height;
          ctx.drawImage(img, 0, 0);

          // Use unified drawing utility
          drawInferenceResults(ctx, result, {
            showLabels: true,
            showConfidence: true,
          });

          resolve();
        };
        img.src = imageUrl;
      });

      galleryImages.push({
        original: imageUrl,
        annotated: canvas.toDataURL(),
        fileName: result.file_name,
        detectionData: result, // Use complete API response
      });
    }

    galleryImages = galleryImages;
    currentGalleryIndex = 0;
  }

  async function inferVideo() {
    if (!selectedFile || !selectedModelId) return;

    // Validate Prompts
    if (promptRequired && inferPrompts.length === 0) {
      uiStore.showWarning(
        "Model that requires prompts requires at least one prompt. Please add a text, point, or box prompt before starting video detection.",
        "Prompts Required",
      );
      return;
    }

    try {
      // Clear previous gallery for video
      galleryImages = [];
      currentGalleryIndex = 0;

      // Get video metadata
      const duration = videoElement.duration || 0;
      const fps = videoElement.videoWidth > 0 ? 30 : 0; // Estimate FPS

      // Build unified inference options
      const options: InferenceConfig = {
        modelId: selectedModelId,
        confidence: confidence,
        campaignId: campaignId,
        duration: duration,
        fps: fps,
        prompts: promptRequired ? inferPrompts : undefined,
        classFilter:
          !promptRequired && classFilter.length > 0 ? classFilter : undefined,
      };

      // Determine capture mode based on UI selection
      const captureMode =
        videoCaptureMode === "continuous" ? "continuous" : "manual";

      // Start video inference job (unified for both modes)
      currentJob = await InferenceAPI.inferVideo(
        selectedFile,
        options,
        captureMode,
        skipFrames,
        undefined, // limitFrames (optional)
      );

      if (videoCaptureMode === "continuous") {
        // Continuous mode: backend processes all frames, just poll for results
        pollVideoJobStatus();
      } else {
        // Manual mode: set up session management and live preview
        activeVideoSession = currentJob;

        // Start heartbeat to keep session alive
        videoSessionHeartbeatInterval = window.setInterval(async () => {
          if (activeVideoSession) {
            try {
              await InferenceAPI.sendJobHeartbeat(activeVideoSession.id);
            } catch (error) {
              console.error("Heartbeat failed:", error);
            }
          }
        }, 30000); // Every 30 seconds

        // Monitor for inactivity warnings
        startInactivityMonitoring();

        // Start live preview with manual capture
        startVideoManualDetection();
      }
    } catch (error) {
      console.error("Video detection failed:", error);
      throw error;
    }
  }

  function startInactivityMonitoring() {
    if (inactivityWarningTimeout) {
      clearInterval(inactivityWarningTimeout);
    }

    inactivityWarningTimeout = window.setInterval(async () => {
      if (activeVideoSession) {
        try {
          const job = await InferenceAPI.getJob(activeVideoSession.id);
          if (job.summary_json?.inactive_warning_shown) {
            showInactivityModal = true;
          }
        } catch (error) {
          console.error("Failed to check session status:", error);
        }
      }
    }, 60000); // Check every minute
  }

  async function finishVideoSession() {
    if (!activeVideoSession) return;

    try {
      // Clear intervals
      if (videoSessionHeartbeatInterval) {
        clearInterval(videoSessionHeartbeatInterval);
        videoSessionHeartbeatInterval = null;
      }
      if (inactivityWarningTimeout) {
        clearInterval(inactivityWarningTimeout);
        inactivityWarningTimeout = null;
      }

      // Finish session on backend
      const completedJob = await InferenceAPI.stopJob(activeVideoSession.id);
      currentJob = completedJob;

      // Clear session state
      activeVideoSession = null;
    } catch (error) {
      console.error("Failed to finish video session:", error);
      uiStore.showError(
        "Failed to finish video session. Please try again.",
        "Video Session Error",
      );
      throw error;
    }
  }

  async function continueSession() {
    showInactivityModal = false;
    if (activeVideoSession) {
      try {
        await InferenceAPI.sendJobHeartbeat(activeVideoSession.id);
      } catch (error) {
        console.error("Failed to continue session:", error);
        uiStore.showError(
          "Failed to continue video session. Please try again.",
          "Video Session Error",
        );
      }
    }
  }

  function startVideoManualDetection(preview_interval_ms?: number) {
    // Auto-detect interval based on model type if not provided
    if (!preview_interval_ms) {
      preview_interval_ms = promptRequired ? 2500 : 1000;
    }

    // Start real-time detection loop for PREVIEW ONLY (no job creation)
    const detectVideoFrame = async () => {
      if (!videoElement || !canvasElement || !selectedModelId) return;

      // Model requires prompts
      if (promptRequired && inferPrompts.length === 0) return;

      try {
        // Draw current video frame to canvas
        const ctx = canvasElement.getContext("2d");
        if (ctx && videoElement.videoWidth > 0) {
          canvasElement.width = videoElement.videoWidth;
          canvasElement.height = videoElement.videoHeight;
          ctx.drawImage(videoElement, 0, 0);

          // Convert canvas to blob
          const blob = await new Promise<Blob | null>((resolve) =>
            canvasElement.toBlob(resolve, "image/jpeg", 0.8),
          );

          if (blob) {
            // Create file from blob
            const file = new File([blob], "frame.jpg", { type: "image/jpeg" });

            // Build API options based on model type
            const options: any = {
              modelId: selectedModelId,
              confidence: confidence,
              classFilter: classFilter.length > 0 ? classFilter : undefined,
            };

            // Add prompts for models that require them
            if (promptRequired) {
              options.prompts = inferPrompts;
            }

            // Run detection for PREVIEW ONLY (no database job created)
            const response = await InferenceAPI.inferPreview(file, options);

            // Draw results on overlay canvas
            if (canvasOverlay) {
              const overlayCtx = canvasOverlay.getContext("2d");
              if (overlayCtx) {
                canvasOverlay.width = videoElement.clientWidth;
                canvasOverlay.height = videoElement.clientHeight;
                overlayCtx.clearRect(
                  0,
                  0,
                  canvasOverlay.width,
                  canvasOverlay.height,
                );

                const scaleX = canvasOverlay.width / videoElement.videoWidth;
                const scaleY = canvasOverlay.height / videoElement.videoHeight;

                // Draw segmentation masks first (if available)
                if (response.masks && response.masks.length > 0) {
                  response.masks.forEach((mask, i) => {
                    // Scale polygon coordinates for overlay canvas
                    const scaledPolygon = mask.polygon.map(([x, y]) => [
                      x * scaleX,
                      y * scaleY,
                    ]);
                    drawPolygonMask(overlayCtx, scaledPolygon, i);
                  });
                }

                // Draw bounding boxes (for detection/classification tasks)
                response.boxes?.forEach((box, i) => {
                  const [x1, y1, x2, y2] = box.map((coord, idx) =>
                    idx % 2 === 0 ? coord * scaleX : coord * scaleY,
                  );
                  const score = response.scores?.[i];
                  const className = response.class_names?.[i];

                  overlayCtx.strokeStyle = "#00ff00";
                  overlayCtx.lineWidth = 2;
                  overlayCtx.strokeRect(x1, y1, x2 - x1, y2 - y1);

                  overlayCtx.fillStyle = "#00ff00";
                  overlayCtx.font = "14px Arial";
                  if (score !== undefined && className) {
                    overlayCtx.fillText(
                      `${className} ${(score * 100).toFixed(1)}%`,
                      x1,
                      y1 - 5,
                    );
                  }
                });
              }
            }

            // Process detection results for stats
            processDetectionResults(response);
          }
        }
      } catch (error) {
        console.error(
          `${promptRequired ? "Model with prompts" : "Model without prompts"} video frame detection failed:`,
          error,
        );
      }
    };

    // Run detection at specified interval for live preview
    videoDetectionIntervalId = window.setInterval(
      detectVideoFrame,
      preview_interval_ms,
    );
    detectVideoFrame(); // Run immediately
  }

  function pollVideoJobStatus() {
    let lastFetchedCount = 0;
    let processedResultIds = new Set<number>();

    pollingIntervalId = window.setInterval(async () => {
      // Stop polling if job was cleared (but allow completion to run)
      if (!currentJob) {
        if (pollingIntervalId !== null) {
          clearInterval(pollingIntervalId);
        }
        return;
      }

      try {
        const job = await InferenceAPI.getJob(currentJob.id);
        currentJob = job;
        console.log(
          `1) Video Job Status: ${job.status}, Results: ${job.results_count || 0}, lastFetched: ${lastFetchedCount}`,
        );

        // Check for completion FIRST, before processing results
        const isComplete =
          job.status === "completed" || job.status === "failed";

        // Fetch new results as they become available
        if (job.results_count && job.results_count > lastFetchedCount) {
          console.log(
            `Fetching results: skip=${lastFetchedCount}, limit=${job.results_count - lastFetchedCount}`,
          );
          const skip = lastFetchedCount;
          const limit = job.results_count - lastFetchedCount;

          try {
            const newResults = await InferenceAPI.getResults(
              job.id,
              skip,
              limit,
            );
            console.log(`Got ${newResults.length} new results`);

            // Add new frames to gallery with deduplication
            for (const result of newResults) {
              // Skip if already processed (prevents duplicates)
              if (processedResultIds.has(result.id || 0)) {
                console.log(`Skipping duplicate result ID ${result.id}`);
                continue;
              }

              console.log(
                `Processing frame ${result.frame_number || "?"} (ID: ${result.id})`,
              );
              processedResultIds.add(result.id || 0);
              await addVideoFrameToGallery(result);
            }

            lastFetchedCount = job.results_count;
            console.log(`Updated lastFetchedCount to ${lastFetchedCount}`);
          } catch (error) {
            console.error("Error processing video frames:", error);
            // Still update lastFetchedCount to avoid infinite loop
            lastFetchedCount = job.results_count;
          }
        }

        console.log(`About to check completion. isComplete=${isComplete}`);

        if (isComplete) {
          // Clear interval first to prevent further polling
          const intervalId = pollingIntervalId;
          if (intervalId !== null) {
            clearInterval(intervalId);
          }

          isDetecting = false;
          currentJob = null;

          console.log(
            `2) Video Job Status: ${job.status}, Results: ${job.results_count || 0}, lastFetched: ${lastFetchedCount}`,
          );

          // Fetch any remaining results before showing completion message
          if (
            job.status === "completed" &&
            job.results_count &&
            job.results_count > lastFetchedCount
          ) {
            const remainingResults = await InferenceAPI.getResults(
              job.id,
              lastFetchedCount,
              job.results_count - lastFetchedCount,
            );

            for (const result of remainingResults) {
              await addVideoFrameToGallery(result);
            }
          }

          console.log(
            `3) Video Job Status: ${job.status}, Results: ${job.results_count || 0}, lastFetched: ${lastFetchedCount}`,
          );

          // Show completion message
          if (job.status === "completed") {
            console.log(
              `4) Video Job Status: ${job.status}, Results: ${job.results_count || 0}, lastFetched: ${lastFetchedCount}`,
            );

            uiStore.showSuccess(
              `Video processing completed. Total frames processed: ${job.results_count || 0}`,
              "Video Job Completed",
            );

            // Start continuous video overlay updates during playback
            const updateOverlayLoop = () => {
              updateVideoOverlay();
              if (
                videoCaptureMode === "continuous" &&
                videoElement &&
                !videoElement.paused
              ) {
                requestAnimationFrame(updateOverlayLoop);
              }
            };
            requestAnimationFrame(updateOverlayLoop);
          } else {
            uiStore.showError(
              `Video processing failed: ${job.error_message || "Unknown error"}`,
              "Video Job Failed",
            );
          }

          console.log(
            `5) Video Job Status: ${job.status}, Results: ${job.results_count || 0}, lastFetched: ${lastFetchedCount}`,
          );
          // Clean up resources
          cleanup();
        }
      } catch (error) {
        console.error("Failed to poll video job status:", error);
      }
    }, 1000); // Poll every second for faster updates
  }

  async function addVideoFrameToGallery(result: any) {
    if (!videoElement || !currentJob) return;

    // Calculate timestamp from frame number using actual video FPS
    const frameNumber = result.frame_number || 0;
    const fps = currentJob.summary_json?.fps || 30;

    // frame_number is the actual frame position in video, so divide by FPS directly
    const timestamp = frameNumber / fps;

    // Create two canvases - one for original, one for annotated
    const originalCanvas = document.createElement("canvas");
    const annotatedCanvas = document.createElement("canvas");
    const originalCtx = originalCanvas.getContext("2d");
    const annotatedCtx = annotatedCanvas.getContext("2d");
    if (!originalCtx || !annotatedCtx) return;

    // Check if we need to seek (skip if already at correct position)
    const needsSeek = Math.abs(videoElement.currentTime - timestamp) > 0.1;

    if (needsSeek) {
      videoElement.currentTime = timestamp;
    }

    try {
      if (needsSeek) {
        await Promise.race([
          new Promise<void>((resolve) => {
            const onSeeked = () => {
              videoElement.removeEventListener("seeked", onSeeked);

              // Set canvas sizes
              originalCanvas.width = annotatedCanvas.width =
                videoElement.videoWidth;
              originalCanvas.height = annotatedCanvas.height =
                videoElement.videoHeight;

              // Draw original frame (no annotations)
              originalCtx.drawImage(videoElement, 0, 0);

              // Draw annotated frame (with bounding boxes)
              annotatedCtx.drawImage(videoElement, 0, 0);
              drawInferenceResults(annotatedCtx, result, {
                showLabels: true,
                showConfidence: true,
              });

              resolve();
            };
            videoElement.addEventListener("seeked", onSeeked);
          }),
          new Promise<void>((_, reject) =>
            setTimeout(() => reject(new Error("Seek timeout")), 5000),
          ),
        ]);
      } else {
        // Already at correct position, just draw
        originalCanvas.width = annotatedCanvas.width = videoElement.videoWidth;
        originalCanvas.height = annotatedCanvas.height =
          videoElement.videoHeight;

        // Draw original frame
        originalCtx.drawImage(videoElement, 0, 0);

        // Draw annotated frame
        annotatedCtx.drawImage(videoElement, 0, 0);
        drawInferenceResults(annotatedCtx, result, {
          showLabels: true,
          showConfidence: true,
        });
      }

      // Add to gallery
      galleryImages = [
        ...galleryImages,
        {
          original: originalCanvas.toDataURL(),
          annotated: annotatedCanvas.toDataURL(),
          fileName: `Frame ${frameNumber}`,
          timestamp: timestamp,
          detectionData: {
            job_id: result.job_id || 0,
            file_name: result.file_name || "",
            boxes: result.boxes,
            scores: result.scores,
            classes: result.classes,
            class_names: result.class_names,
            masks: result.masks || [],
          },
        },
      ];
    } catch (error) {
      console.error(
        `Failed to add video frame ${frameNumber} to gallery:`,
        error,
      );
      // Continue processing other frames even if this one fails
    }
  }

  async function inferRTSP() {
    if (!rtspUrl || !selectedModelId) return;

    // Validate prompts for prompt-required models
    if (promptRequired && inferPrompts.length === 0) {
      uiStore.showWarning(
        'This model requires at least one prompt.\n\nPlease add a text prompt before starting RTSP detection.\nExample: "person", "car", "white bicycle"',
        "Prompt Required",
      );
      isDetecting = false;
      return;
    }

    rtspViewMode = "live"; // Auto-switch to Live tab when starting RTSP Mode

    try {
      var inference_options: InferenceConfig = {
        modelId: selectedModelId,
        confidence: confidence,
        campaignId: campaignId,
        classFilter: classFilter.length > 0 ? classFilter : undefined,
      };

      if (promptRequired) {
        inference_options.prompts = inferPrompts;
      }

      currentJob = await InferenceAPI.inferRTSP(
        rtspUrl,
        inference_options,
        rtspCaptureMode,
        skipFrames,
      );

      if (promptRequired) {
        // Wait for component to receive new jobId prop
        await tick();

        // Start RTSP viewer component streaming (models with requires_prompts)
        if (rtsp_viewer) {
          rtsp_viewer.startStream();
        }
      }

      // Start polling for job status (continuous mode also needs results)
      // Manual mode: only polls job status, not frames (viewer handles frames)
      // Continuous mode: polls both job status and results
      pollJobStatus();
    } catch (error) {
      console.error("RTSP detection failed:", error);
      throw error;
    }
  }

  function pollJobStatus() {
    pollingIntervalId = window.setInterval(async () => {
      if (!currentJob) return;

      try {
        const job = await InferenceAPI.getJob(currentJob.id);
        currentJob = job;

        // For models requiring prompts in continuous mode, fetch results from database
        if (
          promptRequired &&
          rtspCaptureMode === "continuous" &&
          sourceType === "rtsp"
        ) {
          try {
            // Fetch all results from database
            const results = await InferenceAPI.getResults(
              currentJob.id,
              0,
              1000,
            );

            // Add new results to gallery (avoid duplicates)
            const existingFileNames = new Set(
              galleryImages.map((img) => img.fileName),
            );

            for (const result of results) {
              if (!existingFileNames.has(result.file_name)) {
                // Fetch frame image via authenticated endpoint
                const frameResponse = await InferenceAPI.getResultImage(
                  result.id || 0,
                );

                const frameBlob = frameResponse.data;
                const frameUrl = URL.createObjectURL(frameBlob);

                // Draw masks on image
                const img = new Image();
                img.src = frameUrl;

                await new Promise<void>((resolve) => {
                  img.onload = async () => {
                    const canvas = document.createElement("canvas");
                    canvas.width = img.width;
                    canvas.height = img.height;
                    const ctx = canvas.getContext("2d");

                    if (ctx) {
                      // Draw original frame
                      ctx.drawImage(img, 0, 0);
                      if (result.masks && result.masks.length > 0) {
                        result.masks.forEach((mask: MaskData, i) => {
                          drawPolygonMask(ctx, mask.polygon, i);
                        });
                      }

                      const annotatedDataUrl = canvas.toDataURL("image/jpeg");

                      // Add to gallery
                      const detectionData = {
                        job_id: result.job_id || 0,
                        file_name: result.file_name || "",
                        boxes: result.masks?.map((m) => m.bbox || []) || [],
                        scores: result.scores || [],
                        classes: result.classes || [],
                        class_names: result.class_names || [],
                        masks: result.masks || [],
                      };

                      galleryImages = [
                        {
                          original: frameUrl,
                          annotated: annotatedDataUrl,
                          fileName: result.file_name,
                          detectionData,
                        },
                        ...galleryImages,
                      ];

                      resolve();
                    }
                  };
                });
              }
            }
          } catch (error: any) {
            console.error(
              "Error fetching RTSP results for model requiring prompts:",
              error,
            );
          }
        }
        // Fetch latest frame for RTSP manual/continuous capture (standard detection models)
        // Skip frame fetching for models requiring prompts in manual mode (viewer handles it internally)
        else if (
          sourceType === "rtsp" &&
          !(promptRequired && rtspCaptureMode === "manual")
        ) {
          try {
            let frameData: PredictionResponseWithFrame;

            // Use detection API for standard models (not prompt-required)
            if (!promptRequired) {
              frameData = await InferenceAPI.rtsp_get_latest_frame(
                currentJob.id,
              );
              rtspLastFrameData = frameData;
              rtspFrameStatus = "ready";

              // Process prediction results for display
              if (frameData.predictions) {
                processDetectionResults(frameData.predictions);
              }

              // In continuous mode, auto-add frames to gallery
              if (rtspCaptureMode === "continuous") {
                const frameNumber = job.progress || 0;
                if (!processedFrameNumbers.has(frameNumber)) {
                  processedFrameNumbers.add(frameNumber);

                  const img = new Image();
                  img.src = frameData.frame;

                  img.onload = () => {
                    // Check if frame has any predictions before adding to gallery
                    const hasDetections =
                      (frameData?.predictions?.boxes?.length || 0) > 0 ||
                      (frameData?.predictions?.masks?.length || 0) > 0;

                    if (!hasDetections) {
                      return; // Skip frames with no predictions
                    }

                    const canvas = document.createElement("canvas");
                    canvas.width = img.width;
                    canvas.height = img.height;
                    const ctx = canvas.getContext("2d");

                    if (ctx) {
                      ctx.drawImage(img, 0, 0);
                      drawBoundingBoxes(ctx, frameData?.predictions);

                      const annotatedDataUrl = canvas.toDataURL("image/jpeg");

                      galleryImages = [
                        {
                          original: frameData.frame,
                          annotated: annotatedDataUrl,
                          fileName: `RTSP Frame ${frameNumber}`,
                          detectionData: frameData.predictions,
                        },
                        ...galleryImages,
                      ];

                      if (currentGalleryIndex >= galleryImages.length) {
                        currentGalleryIndex = 0;
                      }
                    }
                  };
                }
              }
            }
          } catch (error: any) {
            if (error?.response?.status === 404) {
              rtspFrameStatus = "loading";
            } else {
              rtspFrameStatus = "error";
              console.error("Error fetching RTSP frame:", error);
            }
          }
        }

        if (job.status === "completed" || job.status === "failed") {
          clearInterval(pollingIntervalId);
          isDetecting = false;

          // Fetch and display results for RTSP (skip for models requiring prompts in continuous mode - already loaded in real-time)
          if (
            job.status === "completed" &&
            sourceType === "rtsp" &&
            !(promptRequired && rtspCaptureMode === "continuous")
          ) {
            const results = await InferenceAPI.getResults(job.id, 0, 1000);
            if (results.length > 0) {
              await loadRTSPResultsIntoGallery(results);
            }
          }

          // Auto-switch to gallery when job completes
          if (sourceType === "rtsp" && galleryImages.length > 0) {
            rtspViewMode = "gallery";
          }
        }
      } catch (error) {
        console.error("Failed to poll job status:", error);
      }
    }, 2000);
  }

  async function stopRtspInference() {
    isDetecting = false;

    // Stop RTSP viewer for models requiring prompts if applicable
    if (sourceType === "rtsp" && promptRequired && rtsp_viewer) {
      rtsp_viewer.stopStream();
    }

    // Stop RTSP stream if applicable
    if (sourceType === "rtsp" && currentJob) {
      try {
        await InferenceAPI.stopJob(currentJob.id);

        // For models requiring prompts in continuous mode: fetch final results after stopping
        // (backend worker thread may have processed frames after last poll)
        if (promptRequired && rtspCaptureMode === "continuous") {
          // Wait a bit for backend worker to finish pending frames
          await new Promise((resolve) => setTimeout(resolve, 1000));

          const finalResults = await InferenceAPI.getResults(
            currentJob.id,
            0,
            1000,
          );

          // Add any missing results to gallery
          const existingFileNames = new Set(
            galleryImages.map((img) => img.fileName),
          );

          for (const result of finalResults) {
            if (!existingFileNames.has(result.file_name)) {
              // Fetch frame image
              const frameResponse = await InferenceAPI.getResultImage(
                result.id || 0,
              );
              const frameBlob = frameResponse.data;
              const frameUrl = URL.createObjectURL(frameBlob);

              // Draw masks on image
              const img = new Image();
              img.src = frameUrl;

              await new Promise<void>((resolve) => {
                img.onload = async () => {
                  const canvas = document.createElement("canvas");
                  canvas.width = img.width;
                  canvas.height = img.height;
                  const ctx = canvas.getContext("2d");

                  if (ctx) {
                    ctx.drawImage(img, 0, 0);

                    // Draw polygon masks
                    if (result.masks && result.masks.length > 0) {
                      result.masks.forEach((mask, i) => {
                        drawPolygonMask(ctx, mask.polygon, i);
                      });
                    }
                  }

                  const annotatedDataUrl = canvas.toDataURL("image/jpeg");

                  // Add to gallery
                  const detectionData = {
                    job_id: result.job_id || 0,
                    file_name: result.file_name || "",
                    boxes: result.masks?.map((m) => m.bbox || []) || [],
                    scores: result.scores || [],
                    classes: result.classes || [],
                    class_names: result.class_names || [],
                    masks: result.masks || [],
                  };

                  galleryImages = [
                    {
                      original: frameUrl,
                      annotated: annotatedDataUrl,
                      fileName: result.file_name,
                      detectionData,
                    },
                    ...galleryImages,
                  ];

                  resolve();
                };
              });
            }
          }
        }
      } catch (error) {
        console.error("Failed to stop RTSP stream:", error);
      }
    }

    // Cancel video job for models requiring prompts if applicable
    if (
      promptRequired &&
      sourceType === "video" &&
      videoCaptureMode === "continuous" &&
      currentJob
    ) {
      try {
        await InferenceAPI.cancelJob(currentJob.id);
      } catch (error) {
        console.error("Failed to cancel video job:", error);
        uiStore.showError(
          "Failed to cancel video job. Please try again.",
          "Video Job Error",
        );
      }
    }

    // Finish video session if applicable
    if (
      sourceType === "video" &&
      videoCaptureMode === "manual" &&
      activeVideoSession
    ) {
      try {
        await finishVideoSession(); // Unified for all model types
      } catch (error) {
        console.error("Failed to finish video session:", error);
        uiStore.showError(
          "Failed to finish video session. Please try again.",
          "Video Session Error",
        );
      }
    }

    // Finish webcam session if applicable
    if (sourceType === "webcam" && activeWebcamSession) {
      try {
        await InferenceAPI.stopJob(activeWebcamSession.id);
        activeWebcamSession = null;
      } catch (error) {
        console.error("Failed to finish webcam session:", error);
        uiStore.showError(
          "Failed to finish webcam session. Please try again.",
          "Webcam Session Error",
        );
      }
    }

    cleanup();

    // Auto-switch to gallery mode when stopping (only in continuous mode)
    if (
      sourceType === "webcam" &&
      galleryImages.length > 0 &&
      webcamCaptureMode === "continuous"
    ) {
      webcamViewMode = "gallery";
    } else if (
      sourceType === "rtsp" &&
      galleryImages.length > 0 &&
      rtspCaptureMode === "continuous"
    ) {
      rtspViewMode = "gallery";
    }
  }

  async function inferWebCam() {
    if (!videoElement || !canvasElement || !selectedModelId) return;

    // Initialize webcam stream if not already started
    if (!webcamStream) {
      try {
        webcamStream = await navigator.mediaDevices.getUserMedia({
          video: { width: 640, height: 480 },
        });
        if (videoElement) {
          videoElement.srcObject = webcamStream;
          videoElement.play();
          frameStats.width = 640;
          frameStats.height = 480;
        }
      } catch (error) {
        console.error("Failed to access webcam:", error);
        uiStore.showError(
          "Failed to access webcam. Please check permissions.",
          "Webcam Access Error",
        );
        isDetecting = false;
        return;
      }
    }

    webcamViewMode = "live"; // Auto-switch to Live tab, if camera initialized
    // Start inference session using dedicated webcam endpoint
    activeWebcamSession = await InferenceAPI.inferWebcam(
      {
        modelId: selectedModelId,
        prompts: promptRequired
          ? promptMode === "auto"
            ? []
            : inferPrompts
          : undefined,
        confidence: confidence,
        classFilter: classFilter.length > 0 ? classFilter : undefined,
        campaignId: campaignId,
      },
      webcamCaptureMode,
    );

    if (activeWebcamSession) {
      console.log(`Started webcam session with ID: ${activeWebcamSession.id}`);
      startRealtimeCameraInference();
    } else {
      console.error("Failed to start webcam session.");
      uiStore.showError(
        "Failed to start webcam session. Please try again.",
        "Webcam Session Error",
      );
    }
  }

  function startRealtimeCameraInference() {
    if (!videoElement || !canvasElement || !selectedModelId) return;

    // Validate prompts for continuous mode (required for models requiring prompts)
    if (
      promptRequired &&
      webcamCaptureMode === "continuous" &&
      inferPrompts.length === 0
    ) {
      uiStore.showWarning(
        "Continuous mode requires at least one prompt for this model. Please add a text, point, or box prompt.",
        "Prompt Required",
      );
      isDetecting = false;
      return;
    }

    // Reset to live mode when starting detection
    webcamViewMode = "live";

    let frameCount = 0;
    const captureInterval = promptRequired ? 2500 : 500; // Models requiring prompts: 2.5s, Standard models: 500ms
    let lastCaptureTime = 0;

    const detectFrame = async (timestamp: number) => {
      if (!isDetecting || sourceType !== "webcam") {
        return;
      }

      // Throttle frame capture
      if (timestamp - lastCaptureTime >= captureInterval) {
        lastCaptureTime = timestamp;
        frameCount++;

        try {
          // Check if elements still exist (user might have switched tabs)
          if (!canvasElement || !videoElement) {
            console.warn("Canvas or video element is null, stopping detection");
            isDetecting = false;
            return;
          }

          // Draw video frame to canvas
          const ctx = canvasElement.getContext("2d");
          if (!ctx) return;

          canvasElement.width = videoElement.videoWidth;
          canvasElement.height = videoElement.videoHeight;
          ctx.drawImage(videoElement, 0, 0);

          // Convert canvas to blob
          canvasElement.toBlob(
            async (blob) => {
              if (!blob || !selectedModelId) return;

              try {
                // Convert blob to File for API compatibility
                const file = new File(
                  [blob],
                  `webcam_frame_${frameCount}.jpg`,
                  {
                    type: "image/jpeg",
                  },
                );

                let response = await InferenceAPI.inferPreview(file, {
                  modelId: selectedModelId,
                  prompts: promptRequired ? inferPrompts : undefined,
                  confidence: confidence,
                  classFilter: classFilter.length > 0 ? classFilter : undefined,
                });

                // Process and display results
                processDetectionResults(response);

                // Draw boxes/masks on overlay canvas
                drawWebcamFrameBoxes(response);

                // Cache the last detection response for manual capture
                lastPredictionResponse = response;

                // In continuous mode, automatically save to database and add to gallery
                if (webcamCaptureMode === "continuous" && activeWebcamSession) {
                  try {
                    // Save frame to database
                    if (promptRequired) {
                      // Model requiring prompts: Segment and save
                      canvasElement.toBlob(async (blob) => {
                        if (blob) {
                          const saveFile = new File(
                            [blob],
                            `webcam_auto_${frameCount}.jpg`,
                            { type: "image/jpeg" },
                          );

                          await InferenceAPI.webcam_capture_frame(
                            activeWebcamSession!.id,
                            saveFile,
                            {
                              modelId: selectedModelId || 0,
                              prompts: promptRequired
                                ? inferPrompts
                                : undefined,
                              confidence: confidence,
                              classFilter: classFilter.length
                                ? classFilter
                                : undefined,
                              campaignId: campaignId,
                            },
                            frameCount,
                          );
                        }
                      }, "image/jpeg");
                    } else {
                      // Standard model: Capture frame
                      canvasElement.toBlob(async (blob) => {
                        if (blob) {
                          const saveFile = new File(
                            [blob],
                            `webcam_auto_${frameCount}.jpg`,
                            { type: "image/jpeg" },
                          );

                          await InferenceAPI.webcam_capture_frame(
                            activeWebcamSession!.id,
                            saveFile,
                            {
                              modelId: selectedModelId || 0,
                              prompts: promptRequired
                                ? inferPrompts
                                : undefined,
                              confidence: confidence,
                              classFilter: classFilter.length
                                ? classFilter
                                : undefined,
                              campaignId: campaignId,
                            },
                            frameCount,
                          );
                        }
                      }, "image/jpeg");
                    }

                    // Also add to gallery for immediate display
                    const annotatedCanvas = document.createElement("canvas");
                    annotatedCanvas.width = canvasElement.width;
                    annotatedCanvas.height = canvasElement.height;
                    const annotatedCtx = annotatedCanvas.getContext("2d");

                    if (annotatedCtx) {
                      annotatedCtx.drawImage(canvasElement, 0, 0);

                      if (response.masks && response.masks.length > 0) {
                        // Draw segmentation polygon masks
                        response.masks.forEach((mask: MaskData, i) => {
                          if (mask.polygon && mask.polygon.length > 0) {
                            drawPolygonMask(annotatedCtx, mask.polygon, i);
                          }
                        });
                      } else {
                        // Draw detection bounding boxes
                        drawBoundingBoxes(annotatedCtx, response);
                      }

                      const annotatedDataUrl =
                        annotatedCanvas.toDataURL("image/jpeg");
                      const originalDataUrl =
                        canvasElement.toDataURL("image/jpeg");

                      galleryImages = [
                        {
                          original: originalDataUrl,
                          annotated: annotatedDataUrl,
                          fileName: `Webcam Frame ${frameCount}`,
                          detectionData: response,
                        },
                        ...galleryImages, // Unlimited gallery
                      ];

                      if (currentGalleryIndex >= galleryImages.length) {
                        currentGalleryIndex = 0;
                      }
                    }
                  } catch (error) {
                    console.error("Failed to save continuous frame:", error);
                  }
                }
              } catch (error) {
                console.error("Frame detection failed:", error);
              }
            },
            "image/jpeg",
            0.8,
          );
        } catch (error) {
          console.error("Failed to capture frame:", error);
        }
      }

      // Continue capturing frames
      if (isDetecting && sourceType === "webcam") {
        animationFrameId = requestAnimationFrame(detectFrame);
      }
    };

    // Start the detection loop
    animationFrameId = requestAnimationFrame(detectFrame);
  }

  async function captureCurrentCameraFrame() {
    if (
      !canvasElement ||
      !videoElement ||
      !lastPredictionResponse ||
      !selectedModelId
    )
      return;

    // Validate prompts for manual mode (prompt-required models)
    if (
      promptRequired &&
      webcamCaptureMode === "manual" &&
      inferPrompts.length === 0
    ) {
      uiStore.showWarning(
        "This model requires at least one prompt. Please add a text, point, or box prompt.",
        "Prompt Required",
      );
      return;
    }

    // Trigger flash animation
    isFlashing = true;
    setTimeout(() => {
      isFlashing = false;
    }, 300);

    // Play camera shutter sound
    playShutterSound();

    try {
      // In manual mode, save to database via session capture-frame endpoint
      if (webcamCaptureMode === "manual" && activeWebcamSession) {
        // Convert canvas to blob and save to database
        const blob = await new Promise<Blob | null>((resolve) =>
          canvasElement.toBlob(resolve, "image/jpeg", 0.9),
        );

        if (blob) {
          const file = new File([blob], `webcam_capture_${Date.now()}.jpg`, {
            type: "image/jpeg",
          });

          let savedResponse: PredictionResponse;
          let annotatedDataUrl;
          const originalDataUrl = canvasElement.toDataURL("image/jpeg");

          // Models requiring prompts use the same capture API as standard models. Already unified.
          if (promptRequired) {
            // USE unified Inference API
            savedResponse = await InferenceAPI.video_capture_frame(
              activeWebcamSession.id || 0, // Job Id
              file,
              {
                modelId: selectedModelId,
                prompts: promptRequired ? inferPrompts : undefined,
                confidence: confidence,
                classFilter: classFilter.length ? classFilter : undefined,
                campaignId: campaignId,
              },
              galleryImages.length + 1,
            );

            // Create annotated image with polygon masks
            const annotatedCanvas = document.createElement("canvas");
            annotatedCanvas.width = canvasElement.width;
            annotatedCanvas.height = canvasElement.height;
            const annotatedCtx = annotatedCanvas.getContext("2d");

            if (annotatedCtx) {
              annotatedCtx.drawImage(canvasElement, 0, 0);
              drawBoundingBoxes(annotatedCtx, savedResponse);

              annotatedDataUrl = annotatedCanvas.toDataURL("image/jpeg");
            }
          } else {
            // USE unified Inference API for webcam
            savedResponse = await InferenceAPI.webcam_capture_frame(
              activeWebcamSession.id || 0, // Job Id
              file,
              {
                modelId: selectedModelId,
                prompts: promptRequired ? inferPrompts : undefined,
                confidence: confidence,
                classFilter: classFilter.length ? classFilter : undefined,
                campaignId: campaignId,
              },
              galleryImages.length + 1,
            );

            // Create annotated image with saved response
            const annotatedCanvas = document.createElement("canvas");
            annotatedCanvas.width = canvasElement.width;
            annotatedCanvas.height = canvasElement.height;
            const annotatedCtx = annotatedCanvas.getContext("2d");

            if (annotatedCtx) {
              annotatedCtx.drawImage(canvasElement, 0, 0);
              drawBoundingBoxes(annotatedCtx, savedResponse);

              annotatedDataUrl = annotatedCanvas.toDataURL("image/jpeg");
            }
          }

          galleryImages = [
            {
              original: originalDataUrl,
              annotated: annotatedDataUrl || originalDataUrl,
              fileName: `Captured Frame ${galleryImages.length + 1}`,
              detectionData: savedResponse,
            },
            ...galleryImages,
          ];

          if (currentGalleryIndex >= galleryImages.length) {
            currentGalleryIndex = 0;
          }
        }
      } else {
        // Continuous mode - just add to gallery (already being saved in realtime loop)
        const annotatedCanvas = document.createElement("canvas");
        annotatedCanvas.width = canvasElement.width;
        annotatedCanvas.height = canvasElement.height;
        const annotatedCtx = annotatedCanvas.getContext("2d");

        if (annotatedCtx && lastPredictionResponse) {
          annotatedCtx.drawImage(canvasElement, 0, 0);

          // Check if we have masks (segmentation models) or boxes (detection models)
          if (
            lastPredictionResponse.masks &&
            lastPredictionResponse.masks.length > 0
          ) {
            // Draw polygon masks for models supporting prompts
            if (promptRequired) {
              lastPredictionResponse.masks.forEach((mask: MaskData, i) => {
                if (mask.polygon && mask.polygon.length > 0) {
                  drawPolygonMask(annotatedCtx, mask.polygon, i);
                }
              });
            } else {
              drawBoundingBoxes(annotatedCtx, lastPredictionResponse);
            }
          } else {
            drawBoundingBoxes(annotatedCtx, lastPredictionResponse);
          }

          const annotatedDataUrl = annotatedCanvas.toDataURL("image/jpeg");
          const originalDataUrl = canvasElement.toDataURL("image/jpeg");

          galleryImages = [
            {
              original: originalDataUrl,
              annotated: annotatedDataUrl,
              fileName: `Captured Frame ${galleryImages.length + 1}`,
              detectionData: lastPredictionResponse,
            },
            ...galleryImages,
          ];

          if (currentGalleryIndex >= galleryImages.length) {
            currentGalleryIndex = 0;
          }
        }
      }
    } catch (error) {
      console.error("Failed to capture frame:", error);
      uiStore.showError(
        "Failed to capture frame. Please try again.",
        "Capture Error",
      );
    }
  }

  async function loadRTSPResultsIntoGallery(results: PredictionResult[]) {
    // Load detection results from database into gallery
    // Used when job completes or when results are saved

    for (const result of results) {
      // Check if this result is already in gallery (by file_name)
      const exists = galleryImages.some(
        (img) => img.fileName === result.file_name,
      );
      if (exists) continue;

      try {
        // Construct image URL from detection result
        const imageUrl = `/api/inference/results/${result.id}/image`;

        // Create detection response object
        const detectionData: PredictionResponse = {
          job_id: result.job_id || 0,
          file_name: result.file_name || "",
          boxes: result.boxes || [],
          scores: result.scores || [],
          classes: result.classes || [],
          class_names: result.class_names || [],
          masks: result.masks || [],
        };

        // Load and create annotated image
        const img = new Image();
        img.crossOrigin = "anonymous";

        await new Promise((resolve, reject) => {
          img.onload = () => {
            const canvas = document.createElement("canvas");
            canvas.width = img.width;
            canvas.height = img.height;
            const ctx = canvas.getContext("2d");

            if (ctx) {
              // Draw original image
              ctx.drawImage(img, 0, 0);

              // Use unified drawing utility (works for all models)
              drawInferenceResults(ctx, detectionData, {
                showLabels: true,
                showConfidence: true,
              });

              const annotatedDataUrl = canvas.toDataURL("image/jpeg");

              // Add to gallery
              galleryImages = [
                ...galleryImages,
                {
                  original: imageUrl,
                  annotated: annotatedDataUrl,
                  fileName: result.file_name,
                  detectionData: detectionData,
                },
              ];
            }
            resolve(null);
          };
          img.onerror = reject;
          img.src = imageUrl;
        });
      } catch (error) {
        console.error(`Failed to load result ${result.id}:`, error);
      }
    }

    // Reset gallery index if needed
    if (
      currentGalleryIndex >= galleryImages.length &&
      galleryImages.length > 0
    ) {
      currentGalleryIndex = 0;
    }
  }

  // Handler for RTSPViewer capture event
  async function handleRTSPCapture(
    event: CustomEvent<{ frame: string; masks: any[] }>,
  ) {
    if (!currentJob) return;

    try {
      // Trigger flash animation
      isFlashing = true;
      setTimeout(() => {
        isFlashing = false;
      }, 300);

      // Play camera shutter sound
      playShutterSound();

      // Call backend to capture and process frame with model requiring prompts
      const response = await InferenceAPI.rtsp_capture_frame(currentJob.id, {
        modelId: 0, // Model ID is not needed for RTSP capture (already in job context)
        prompts: inferPrompts,
        campaignId: campaignId,
        confidence: confidence,
        classFilter: classFilter.length > 0 ? classFilter : undefined,
      });

      // Add to gallery with backend response
      const detectionData = {
        job_id: response.job_id || 0,
        file_name: response.file_name || "",
        boxes: response.masks?.map((m) => m.bbox || []) || [],
        scores: response.masks?.map((m) => m.score) || [],
        classes: response.masks?.map((m) => m.class_id) || [],
        class_names: response.masks?.map((m) => m.class_name) || [],
        masks: response.masks || [],
      };

      // Draw masks on image
      const img = new Image();
      img.src = response.frame_base64 || "";

      const annotatedDataUrl = await new Promise<string>((resolve) => {
        img.onload = () => {
          const canvas = document.createElement("canvas");
          canvas.width = img.width;
          canvas.height = img.height;
          const ctx = canvas.getContext("2d");

          if (ctx) {
            // Draw original frame
            ctx.drawImage(img, 0, 0);

            // Draw polygon masks
            if (response.masks && response.masks.length > 0) {
              response.masks.forEach((mask, i) => {
                drawPolygonMask(ctx, mask.polygon, i);
              });
            }
          }

          resolve(canvas.toDataURL("image/jpeg"));
        };
      });

      galleryImages = [
        {
          original: response.frame_base64 || "",
          annotated: annotatedDataUrl,
          fileName:
            response.file_name || `RTSP Frame ${galleryImages.length + 1}`,
          detectionData,
        },
        ...galleryImages,
      ];
    } catch (error) {
      console.error("Failed to capture RTSP frame:", error);
      uiStore.showError(
        "Failed to capture RTSP frame. Please try again.",
        "RTSP Capture Error",
      );
      isFlashing = false;
    }
  }

  async function captureCurrentRTSPFrame() {
    if (!currentJob || !rtspLastFrameData) return;

    // Validate prompts for manual mode
    if (
      promptRequired &&
      rtspCaptureMode === "manual" &&
      inferPrompts.length === 0
    ) {
      uiStore.showWarning(
        "This model requires at least one prompt. Please add a text, point, or box prompt.",
        "Prompt Required",
      );
      return;
    }

    const frameData = rtspLastFrameData; // Capture reference for closure

    // Trigger flash animation
    isFlashing = true;
    setTimeout(() => {
      isFlashing = false;
    }, 300);

    // Play camera shutter sound
    playShutterSound();

    try {
      // Convert base64 to blob
      const response = await fetch(frameData.frame);
      const blob = await response.blob();
      const file = new File([blob], `rtsp_frame_${Date.now()}.jpg`, {
        type: "image/jpeg",
      });

      let savedResponse: PredictionResponse;
      let inference_options: InferenceConfig = {
        modelId: selectedModelId || 0,
        confidence: confidence,
        classFilter: classFilter.length ? classFilter : undefined,
        campaignId: campaignId,
      };

      if (promptRequired) {
        inference_options.prompts = inferPrompts;
      }

      if (rtspCaptureMode === "manual" && currentJob.id > 0) {
        savedResponse = await InferenceAPI.rtsp_capture_frame(
          currentJob.id,
          inference_options,
        );
      }

      if (promptRequired) {
        // Update gallery with segmentation masks
        const img = new Image();
        img.src = frameData.frame;

        await new Promise((resolve) => {
          img.onload = () => {
            const canvas = document.createElement("canvas");
            canvas.width = img.width;
            canvas.height = img.height;
            const ctx = canvas.getContext("2d");

            if (ctx) {
              // Draw original frame
              ctx.drawImage(img, 0, 0);

              // Draw polygon masks
              if (savedResponse.masks && savedResponse.masks.length > 0) {
                savedResponse.masks.forEach((mask, i) => {
                  drawPolygonMask(ctx, mask.polygon, i);
                });
              }

              const annotatedDataUrl = canvas.toDataURL("image/jpeg");

              // Add to gallery
              galleryImages = [
                {
                  original: frameData.frame,
                  annotated: annotatedDataUrl,
                  fileName: `RTSP Frame ${galleryImages.length + 1}`,
                  detectionData: savedResponse,
                },
                ...galleryImages,
              ];

              if (currentGalleryIndex >= galleryImages.length) {
                currentGalleryIndex = 0;
              }

              // Show success notification
              uiStore.showError(
                "Failed to capture RTSP frame. Please try again.",
                "RTSP Capture Error",
              );
            }
            resolve(null);
          };
        });
      } else {
        // Update gallery with saved frame info
        const img = new Image();
        img.src = frameData.frame;

        await new Promise((resolve) => {
          img.onload = () => {
            const canvas = document.createElement("canvas");
            canvas.width = img.width;
            canvas.height = img.height;
            const ctx = canvas.getContext("2d");

            if (ctx) {
              // Draw original frame
              ctx.drawImage(img, 0, 0);

              // Draw bounding boxes
              drawBoundingBoxes(ctx, savedResponse);

              const annotatedDataUrl = canvas.toDataURL("image/jpeg");

              // Add to gallery
              galleryImages = [
                {
                  original: frameData.frame,
                  annotated: annotatedDataUrl,
                  fileName: `RTSP Frame ${galleryImages.length + 1}`,
                  detectionData: savedResponse,
                },
                ...galleryImages,
              ];

              if (currentGalleryIndex >= galleryImages.length) {
                currentGalleryIndex = 0;
              }

              // Show success notification
              uiStore.showSuccess(
                `Frame captured! Total: ${galleryImages.length}`,
                "Capture Success",
              );
            }
            resolve(null);
          };
        });
      }
    } catch (error) {
      console.error("Failed to capture RTSP frame:", error);
      uiStore.showError(
        "Failed to capture frame. Please try again.",
        "Capture Error",
      );
    }
  }

  function playShutterSound() {
    try {
      const audioContext = new (window.AudioContext ||
        (window as any).webkitAudioContext)();
      const oscillator = audioContext.createOscillator();
      const gainNode = audioContext.createGain();

      oscillator.connect(gainNode);
      gainNode.connect(audioContext.destination);

      oscillator.frequency.value = 800;
      oscillator.type = "sine";

      gainNode.gain.setValueAtTime(0.3, audioContext.currentTime);
      gainNode.gain.exponentialRampToValueAtTime(
        0.01,
        audioContext.currentTime + 0.1,
      );

      oscillator.start(audioContext.currentTime);
      oscillator.stop(audioContext.currentTime + 0.1);
    } catch (error) {
      console.warn("Audio feedback not available:", error);
    }
  }

  async function captureCurrentVideoFrame() {
    if (!videoElement || !activeVideoSession || !selectedModelId) return;

    // Validate prompts for prompt-required models
    if (promptRequired && inferPrompts.length === 0) {
      uiStore.showWarning(
        "This model requires at least one prompt. Please add a text, point, or box prompt.",
        "Prompt Required",
      );
      return;
    }

    // Auto-pause video on capture
    const wasPlaying = !videoElement.paused;
    if (wasPlaying) {
      videoElement.pause();
    }

    // Trigger flash animation
    isFlashing = true;
    setTimeout(() => {
      isFlashing = false;
    }, 300);

    // Play camera shutter sound
    playShutterSound();

    try {
      // Capture current frame from video
      const canvas = document.createElement("canvas");
      canvas.width = videoElement.videoWidth;
      canvas.height = videoElement.videoHeight;
      const ctx = canvas.getContext("2d");

      if (ctx) {
        // Draw current video frame
        ctx.drawImage(videoElement, 0, 0);

        // Convert canvas to blob
        const blob = await new Promise<Blob | null>((resolve) =>
          canvas.toBlob(resolve, "image/jpeg", 0.9),
        );

        if (blob) {
          // Create file from blob with timestamp
          const currentTime = videoElement.currentTime;
          const frameNumber = Math.floor(currentTime * 30); // Estimate frame number (30 fps)
          const timestamp = formatTimestamp(currentTime);
          const file = new File([blob], `frame_${frameNumber}.jpg`, {
            type: "image/jpeg",
          });

          let response: PredictionResponse;
          let originalDataUrl = canvas.toDataURL("image/jpeg");
          let annotatedDataUrl = originalDataUrl;

          if (promptRequired) {
            // Unified Video Frame Capture API
            response = await InferenceAPI.video_capture_frame(
              activeVideoSession.id,
              file,
              {
                modelId: selectedModelId, // Not used in backend but kept for consistency
                prompts: inferPrompts,
                confidence: confidence || 0,
                classFilter: classFilter.length > 0 ? classFilter : undefined,
              },
              frameNumber,
              timestamp,
            );

            // Draw bounding boxes
            drawBoundingBoxes(ctx, response);

            // Get annotated frame as data URL
            annotatedDataUrl = canvas.toDataURL("image/jpeg");
          } else {
            // Unified Video Frame Capture API
            response = await InferenceAPI.video_capture_frame(
              activeVideoSession.id,
              file,
              {
                modelId: selectedModelId, // Not used in backend but kept for consistency
                confidence: confidence || 0,
                classFilter: classFilter.length > 0 ? classFilter : undefined,
              },
              frameNumber,
              timestamp,
            );

            // Draw bounding boxes
            drawBoundingBoxes(ctx, response);

            // Get annotated frame as data URL
            annotatedDataUrl = canvas.toDataURL("image/jpeg");
          }

          // Add to gallery
          galleryImages = [
            {
              original: originalDataUrl,
              annotated: annotatedDataUrl,
              fileName: `Frame at ${timestamp} (${galleryImages.length + 1})`,
              detectionData: response,
              timestamp: currentTime,
            },
            ...galleryImages,
          ];

          if (currentGalleryIndex >= galleryImages.length) {
            currentGalleryIndex = 0;
          }
        }
      }
    } catch (error) {
      console.error("Failed to capture frame:", error);
      uiStore.showError(
        "Failed to capture frame. Please try again.",
        "Capture Error",
      );
    }
  }

  function formatTimestamp(seconds: number): string {
    const hours = Math.floor(seconds / 3600);
    const minutes = Math.floor((seconds % 3600) / 60);
    const secs = Math.floor(seconds % 60);
    const ms = Math.floor((seconds % 1) * 10);

    if (hours > 0) {
      return `${hours.toString().padStart(2, "0")}:${minutes.toString().padStart(2, "0")}:${secs.toString().padStart(2, "0")}.${ms}`;
    }
    return `${minutes.toString().padStart(2, "0")}:${secs.toString().padStart(2, "0")}.${ms}`;
  }

  // Continuously update video overlay with bounding boxes during playback
  function updateVideoOverlay() {
    if (
      !videoElement ||
      !canvasOverlay ||
      sourceType !== "video" ||
      videoCaptureMode !== "continuous" ||
      galleryImages.length === 0
    ) {
      return;
    }

    const currentTime = videoElement.currentTime;

    // Find the closest gallery item to current video timestamp
    let closestItem = galleryImages[0];
    let minDiff = Math.abs((closestItem.timestamp || 0) - currentTime);

    for (const item of galleryImages) {
      const diff = Math.abs((item.timestamp || 0) - currentTime);
      if (diff < minDiff) {
        minDiff = diff;
        closestItem = item;
      }
    }

    // Only draw if we have a reasonably close match (within 1 second)
    if (minDiff < 1.0 && closestItem.detectionData) {
      drawVideoFrameBoxes(closestItem.detectionData);
    } else {
      // Clear overlay if no close match
      const ctx = canvasOverlay.getContext("2d");
      if (ctx) {
        ctx.clearRect(0, 0, canvasOverlay.width, canvasOverlay.height);
      }
    }
  }

  function drawWebcamFrameBoxes(response: PredictionResponse) {
    if (!canvasOverlay || !videoElement) return;

    const ctx = canvasOverlay.getContext("2d");
    if (!ctx) return;

    // Additional check for video dimensions (might be 0 if video not ready or tab switched)
    if (!videoElement.videoWidth || !videoElement.videoHeight) {
      console.warn("Video dimensions not available yet");
      return;
    }

    // Set canvas size to match displayed video element size
    const rect = videoElement.getBoundingClientRect();
    canvasOverlay.width = rect.width;
    canvasOverlay.height = rect.height;

    // Clear previous boxes
    ctx.clearRect(0, 0, canvasOverlay.width, canvasOverlay.height);

    // Calculate scaling factors
    const scaleX = rect.width / videoElement.videoWidth;
    const scaleY = rect.height / videoElement.videoHeight;

    // Draw bounding boxes with scaling
    drawBoundingBoxesScaled(ctx, response, scaleX, scaleY);
  }

  function drawRTSPLiveFrame(frameData: PredictionResponseWithFrame) {
    if (!rtspCanvasElement || !rtspCanvasOverlay) return;

    const img = new Image();
    img.onload = () => {
      if (!rtspCanvasElement || !rtspCanvasOverlay) return; // Re-check in case of async load

      // Draw frame on main canvas
      const ctx = rtspCanvasElement.getContext("2d");
      if (ctx) {
        rtspCanvasElement.width = img.width;
        rtspCanvasElement.height = img.height;
        ctx.drawImage(img, 0, 0);
      }

      // Draw detections on overlay canvas
      const overlayCtx = rtspCanvasOverlay.getContext("2d");
      if (overlayCtx) {
        rtspCanvasOverlay.width = img.width;
        rtspCanvasOverlay.height = img.height;
        overlayCtx.clearRect(
          0,
          0,
          rtspCanvasOverlay.width,
          rtspCanvasOverlay.height,
        );
        drawBoundingBoxes(overlayCtx, frameData.predictions);
      }
    };
    img.src = frameData.frame;
  }

  function drawBoundingBoxesScaled(
    ctx: CanvasRenderingContext2D,
    response: PredictionResponse,
    scaleX: number,
    scaleY: number,
  ) {
    // Use unified drawing utility with scaling
    drawInferenceResultsScaled(ctx, response, scaleX, scaleY, {
      showLabels: true,
      showConfidence: true,
      selectedClasses: selectedClasses.size > 0 ? selectedClasses : undefined,
    });
  }

  function drawBoundingBoxes(
    ctx: CanvasRenderingContext2D,
    response: PredictionResponse,
  ) {
    // Use unified drawing utility
    drawInferenceResults(ctx, response, {
      showLabels: true,
      showConfidence: true,
      selectedClasses: selectedClasses.size > 0 ? selectedClasses : undefined,
    });
  }

  function processDetectionResults(response: PredictionResponse) {
    // Use unified processing utility
    const processed = processInferenceResponse(
      response,
      frameStats.width,
      frameStats.height,
      frameStats.fps,
    );

    detectionResults = processed.detectionResults;
    classCounts = processed.classCounts;
    availableClasses = processed.availableClasses;
    // Update frame stats if needed
    if (processed.frameStats) {
      frameStats = processed.frameStats;
    }
  }

  function updateCurrentFrameStats(detectionData: PredictionResponse) {
    // Use unified processing utility
    const canvasWidth = canvasElement?.width || 0;
    const canvasHeight = canvasElement?.height || 0;
    const videoFps = videoElement?.playbackRate ? 30 : undefined;

    const processed = processInferenceResponse(
      detectionData,
      canvasWidth,
      canvasHeight,
      videoFps,
    );

    detectionResults = processed.detectionResults;
    classCounts = processed.classCounts;
    availableClasses = processed.availableClasses;
    frameStats = processed.frameStats;

    // Draw bounding boxes on video canvas if in video mode
    if (sourceType === "video" && canvasElement && videoElement) {
      drawVideoFrameBoxes(detectionData);
    }
  }

  function drawVideoFrameBoxes(detectionData: PredictionResponse) {
    if (!canvasOverlay || !videoElement) return;

    const ctx = canvasOverlay.getContext("2d");
    if (!ctx) return;

    // Set overlay canvas size to match displayed video element size
    const rect = videoElement.getBoundingClientRect();
    canvasOverlay.width = rect.width;
    canvasOverlay.height = rect.height;

    // Clear previous drawings
    ctx.clearRect(0, 0, canvasOverlay.width, canvasOverlay.height);

    // Calculate scaling factors
    const scaleX = rect.width / videoElement.videoWidth;
    const scaleY = rect.height / videoElement.videoHeight;

    // Draw bounding boxes with scaling
    drawInferenceResultsScaled(ctx, detectionData, scaleX, scaleY, {
      showLabels: true,
      showConfidence: true,
      selectedClasses: selectedClasses.size > 0 ? selectedClasses : undefined,
    });
  }

  function drawDetections() {
    if (!canvasElement || !detections || !imagePreview) return;

    const ctx = canvasElement.getContext("2d");
    if (!ctx) return;

    const img = new Image();
    img.onload = () => {
      canvasElement.width = img.width;
      canvasElement.height = img.height;
      ctx.drawImage(img, 0, 0);

      // Use unified drawing utility
      drawInferenceResults(ctx, detections!, {
        showLabels: true,
        showConfidence: true,
        selectedClasses: selectedClasses.size > 0 ? selectedClasses : undefined,
      });
    };
    img.src = imagePreview;
  }

  async function drawDetectionsToDataURL(): Promise<string> {
    if (!detections || !imagePreview) return "";

    const img = new Image();
    img.src = imagePreview;
    await new Promise((resolve) => (img.onload = resolve));

    // Use unified drawing utility to create data URL
    return await drawInferenceResultsToDataURL(img, detections, {
      showLabels: true,
      showConfidence: true,
      selectedClasses: selectedClasses.size > 0 ? selectedClasses : undefined,
    });
  }

  function drawInferencePrompts() {
    if (!canvasElement || !promptRequired) return;

    const ctx = canvasElement.getContext("2d");
    if (!ctx) return;

    const rect = canvasElement.getBoundingClientRect();
    const canvasWidth = canvasElement.width;
    const canvasHeight = canvasElement.height;
    const rectWidth = rect.width;
    const rectHeight = rect.height;

    // Draw existing prompts
    inferPrompts.forEach((prompt) => {
      if (prompt.type === "point") {
        // Scale coordinates for rendering
        if (!prompt.coords || prompt.coords.length < 2) return;
        const coords = prompt.coords;
        const x = (coords[0] * rectWidth) / canvasWidth;
        const y = (coords[1] * rectHeight) / canvasHeight;

        ctx.fillStyle = prompt.label === 1 ? "#22c55e" : "#ef4444"; // Green for foreground, red for background
        ctx.beginPath();
        ctx.arc(x, y, 6, 0, Math.PI * 2);
        ctx.fill();

        // Draw outline
        ctx.strokeStyle = "#ffffff";
        ctx.lineWidth = 2;
        ctx.stroke();
      } else if (prompt.type === "box") {
        // Scale coordinates for rendering
        if (!prompt.coords || prompt.coords.length < 4) return;
        const coords = prompt.coords;
        const x1 = (coords[0] * rectWidth) / canvasWidth;
        const y1 = (coords[1] * rectHeight) / canvasHeight;
        const x2 = (coords[2] * rectWidth) / canvasWidth;
        const y2 = (coords[3] * rectHeight) / canvasHeight;

        ctx.strokeStyle = "#3b82f6"; // Blue for boxes
        ctx.lineWidth = 3;
        ctx.strokeRect(x1, y1, x2 - x1, y2 - y1);

        // Draw corner handles
        const handleSize = 8;
        ctx.fillStyle = "#3b82f6";
        [
          [x1, y1],
          [x2, y1],
          [x1, y2],
          [x2, y2],
        ].forEach(([hx, hy]) => {
          ctx.fillRect(
            hx - handleSize / 2,
            hy - handleSize / 2,
            handleSize,
            handleSize,
          );
        });
      }
    });

    // Draw temp box preview
    if (tempBoxCoords && isDrawingBox) {
      const x1 = (tempBoxCoords.x1 * rectWidth) / canvasWidth;
      const y1 = (tempBoxCoords.y1 * rectHeight) / canvasHeight;
      const x2 = (tempBoxCoords.x2 * rectWidth) / canvasWidth;
      const y2 = (tempBoxCoords.y2 * rectHeight) / canvasHeight;

      ctx.strokeStyle = "#3b82f6";
      ctx.lineWidth = 2;
      ctx.setLineDash([5, 5]);
      ctx.strokeRect(x1, y1, x2 - x1, y2 - y1);
      ctx.setLineDash([]);
    }
  }

  function resetDetection() {
    cleanup();
    selectedFile = null;
    selectedFiles = [];
    imagePreview = null;
    detections = null;
    detectionResults = [];
    rtspLastFrameData = null;
    processedFrameNumbers.clear();
    classCounts = {};
    currentJob = null;
    isDetecting = false;
    rtspUrl = "";
    galleryImages = [];
    currentGalleryIndex = 0;

    // Clear file input to allow re-selection
    if (fileInputElement) {
      fileInputElement.value = "";
    }

    if (canvasElement) {
      const ctx = canvasElement.getContext("2d");
      if (ctx) {
        ctx.clearRect(0, 0, canvasElement.width, canvasElement.height);
      }
    }
  }

  function cleanup() {
    if (animationFrameId) {
      cancelAnimationFrame(animationFrameId);
    }
    if (pollingIntervalId) {
      clearInterval(pollingIntervalId);
    }
    if (videoDetectionIntervalId) {
      clearInterval(videoDetectionIntervalId);
      videoDetectionIntervalId = null;
    }
    if (webcamStream) {
      webcamStream.getTracks().forEach((track) => track.stop());
      webcamStream = null;
    }
  }

  function toggleClassFilter(className: string) {
    if (selectedClasses.has(className)) {
      selectedClasses.delete(className);
    } else {
      selectedClasses.add(className);
    }
    selectedClasses = selectedClasses;
    drawDetections();
  }

  function downloadResults() {
    // Placeholder for download functionality
    if (canvasElement) {
      const link = document.createElement("a");
      link.download = "detection_result.png";
      link.href = canvasElement.toDataURL();
      link.click();
    }
  }

  function captureFrame() {
    // Placeholder for frame capture
    downloadResults();
  }
</script>

<div class="detection-page">
  <!-- Header -->
  <header class="detection-header">
    <div class="header-left">
      <h1>New Prediction</h1>
    </div>
  </header>

  <!-- Main Content -->
  <div class="detection-content">
    <!-- Video/Image Area -->
    <div class="media-area">
      <!-- Mode Switchers -->
      {#if $useNewModes}
        <!-- Phase 2: New ViewModeSwitcher Component -->
        {#if sourceType === "rtsp"}
          <ViewModeSwitcher
            sourceType="rtsp"
            viewMode={rtspViewMode}
            galleryCount={galleryImages.length}
            {isDetecting}
            isRecording={isDetecting && rtspViewMode !== "live"}
            onModeChange={(detail) => (rtspViewMode = detail.mode)}
          />
        {:else if sourceType === "webcam"}
          <ViewModeSwitcher
            sourceType="webcam"
            viewMode={webcamViewMode}
            galleryCount={galleryImages.length}
            {isDetecting}
            isRecording={isDetecting && webcamViewMode !== "live"}
            onModeChange={(detail) => (webcamViewMode = detail.mode)}
          />
        {/if}
      {:else}
        <!-- Legacy: Original Mode Switchers -->
        <!-- RTSP Mode Switcher (iPhone-style) -->
        {#if sourceType === "rtsp"}
          <div class="webcam-mode-switcher">
            <div class="segmented-control">
              <button
                class="segment-button"
                class:active={rtspViewMode === "live"}
                on:click={() => (rtspViewMode = "live")}
                disabled={isDetecting}
              >
                <span class="segment-icon">🔴</span>
                <span class="segment-label">Live</span>
                {#if isDetecting && rtspViewMode !== "live"}
                  <span class="rec-badge">REC</span>
                {/if}
              </button>
              <button
                class="segment-button"
                class:active={rtspViewMode === "gallery"}
                on:click={() => (rtspViewMode = "gallery")}
                disabled={galleryImages.length === 0 || isDetecting}
              >
                <span class="segment-icon">📸</span>
                <span class="segment-label">History</span>
                {#if galleryImages.length > 0}
                  <span class="count-badge">{galleryImages.length}</span>
                {/if}
              </button>
            </div>
          </div>
        {/if}
        <!-- Webcam Mode Switcher (iPhone-style) -->
        {#if sourceType === "webcam"}
          <div class="webcam-mode-switcher">
            <div class="segmented-control">
              <button
                class="segment-button"
                class:active={webcamViewMode === "live"}
                on:click={() => (webcamViewMode = "live")}
                disabled={isDetecting}
              >
                <span class="segment-icon">🔴</span>
                <span class="segment-label">Live</span>
                {#if isDetecting && webcamViewMode !== "live"}
                  <span class="rec-badge">REC</span>
                {/if}
              </button>
              <button
                class="segment-button"
                class:active={webcamViewMode === "gallery"}
                on:click={() => (webcamViewMode = "gallery")}
                disabled={galleryImages.length === 0 || isDetecting}
              >
                <span class="segment-icon">📸</span>
                <span class="segment-label">History</span>
                {#if galleryImages.length > 0}
                  <span class="count-badge">{galleryImages.length}</span>
                {/if}
              </button>
            </div>
          </div>
        {/if}
      {/if}

      {#if $useNewModes}
        <!-- Phase 2: New MediaDisplay Component -->
        <MediaDisplay
          {sourceType}
          {galleryImages}
          {currentGalleryIndex}
          {zoomLevel}
          bind:panOffset
          bind:isPanning
          bind:panStart
          bind:videoElement
          bind:canvasElement
          bind:canvasOverlay
          bind:isFlashing
          {rtspViewMode}
          bind:rtspCanvasElement
          bind:rtspCanvasOverlay
          {rtspFrameStatus}
          {rtspCaptureMode}
          {promptRequired}
          bind:rtsp_viewer
          {rtspUrl}
          {selectedModelId}
          {inferPrompts}
          {webcamViewMode}
          {videoCaptureMode}
          onUpdateOverlay={updateVideoOverlay}
        />
      {:else}
        <!-- Legacy: Original Media Container -->
        <div class="media-container">
          {#if galleryImages.length > 0 && (sourceType === "image" || sourceType === "batch")}
            <!-- Show gallery image in main preview -->
            <div
              class="main-preview"
              class:zoomed={zoomLevel > 1}
              on:mousedown={(e) => {
                if (zoomLevel > 1) {
                  isPanning = true;
                  panStart = {
                    x: e.clientX - panOffset.x,
                    y: e.clientY - panOffset.y,
                  };
                }
              }}
              on:mousemove={(e) => {
                if (isPanning && zoomLevel > 1) {
                  panOffset = {
                    x: e.clientX - panStart.x,
                    y: e.clientY - panStart.y,
                  };
                }
              }}
              on:mouseup={() => {
                isPanning = false;
              }}
              on:mouseleave={() => {
                isPanning = false;
              }}
            >
              <img
                src={galleryImages[currentGalleryIndex].annotated}
                alt="Detection result"
                style="transform: scale({zoomLevel}) translate({panOffset.x /
                  zoomLevel}px, {panOffset.y /
                  zoomLevel}px); cursor: {zoomLevel > 1
                  ? isPanning
                    ? 'grabbing'
                    : 'grab'
                  : 'default'};"
              />
            </div>

            <!-- Navigation arrows overlay on main preview -->
            {#if galleryImages.length > 1}
              <button
                class="gallery-nav gallery-nav-prev"
                on:click={() =>
                  (currentGalleryIndex = Math.max(0, currentGalleryIndex - 1))}
                disabled={currentGalleryIndex === 0}
              >
                ‹
              </button>
              <button
                class="gallery-nav gallery-nav-next"
                on:click={() =>
                  (currentGalleryIndex = Math.min(
                    galleryImages.length - 1,
                    currentGalleryIndex + 1,
                  ))}
                disabled={currentGalleryIndex === galleryImages.length - 1}
              >
                ›
              </button>
            {/if}
          {:else if imagePreview && sourceType === "image"}
            <canvas
              bind:this={canvasElement}
              class="detection-canvas {promptRequired && promptMode !== 'auto'
                ? 'segment-canvas'
                : ''}"
              on:click={handleCanvasClick}
              on:mousemove={handleCanvasMouseMove}
            ></canvas>
          {:else if sourceType === "rtsp" && currentJob && rtspViewMode === "live"}
            <!-- RTSP Live View -->
            {#if promptRequired}
              <!-- RTSP Viewer Component (Models Requiring Prompts) -->
              <RTSPViewer
                bind:this={rtsp_viewer}
                modelId={selectedModelId || 0}
                {rtspUrl}
                prompts={inferPrompts}
                captureMode={rtspCaptureMode}
                {skipFrames}
                {campaignId}
                jobId={currentJob?.id || 0}
                on:captureFrame={handleRTSPCapture}
                on:error={(e) => console.error("RTSP Error:", e.detail)}
              />
            {:else}
              <!-- RTSP Canvas View (Standard Rendering) -->
              <div class="rtsp-stream-container" style="position: relative;">
                <canvas bind:this={rtspCanvasElement} class="video-display"
                ></canvas>
                <canvas
                  bind:this={rtspCanvasOverlay}
                  class="detection-canvas overlay"
                ></canvas>
                {#if isFlashing}
                  <div class="camera-flash-overlay"></div>
                {/if}
                {#if rtspFrameStatus === "loading" && rtspCaptureMode === "manual"}
                  <div class="rtsp-frame-status-overlay">
                    <div class="status-message loading">
                      <div class="spinner"></div>
                      <p>Waiting for video stream...</p>
                      <small>No frame available yet</small>
                    </div>
                  </div>
                {/if}
              </div>
            {/if}
          {:else if sourceType === "video" || (sourceType === "webcam" && webcamViewMode === "live")}
            <video
              bind:this={videoElement}
              class="video-display"
              controls={sourceType === "video"}
              on:seeked={() => {
                // Update overlay when user seeks in continuous mode
                if (
                  videoCaptureMode === "continuous" &&
                  galleryImages.length > 0
                ) {
                  updateVideoOverlay();
                }
              }}
              on:timeupdate={() => {
                // Update overlay during playback in continuous mode
                if (
                  videoCaptureMode === "continuous" &&
                  galleryImages.length > 0 &&
                  !videoElement.paused
                ) {
                  updateVideoOverlay();
                }
              }}
              on:keydown={(e) => {
                // Prevent space bar from controlling video playback in manual capture mode
                if (
                  e.code === "Space" &&
                  sourceType === "video" &&
                  videoCaptureMode === "manual" &&
                  isDetecting
                ) {
                  e.preventDefault();
                  e.stopPropagation();
                }
              }}
            >
              <track kind="captions" />
            </video>
            <canvas
              bind:this={canvasElement}
              class="detection-canvas overlay"
              style="display: none;"
            ></canvas>
            <canvas bind:this={canvasOverlay} class="detection-canvas overlay"
            ></canvas>
            {#if isFlashing && (sourceType === "webcam" || sourceType === "video")}
              <div class="camera-flash-overlay"></div>
            {/if}
          {:else if sourceType === "rtsp" && rtspViewMode === "gallery" && !$useNewComponents}
            <!-- LEGACY: RTSP Gallery View -->
            <div class="gallery-view-container">
              {#if galleryImages.length > 0}
                <!-- Main Preview -->
                <div class="gallery-main-preview">
                  <img
                    src={galleryImages[selectedImageIndex]?.annotated ||
                      galleryImages[selectedImageIndex]?.original}
                    alt="Selected frame"
                    class="gallery-main-image"
                  />
                  <div class="gallery-main-info">
                    <p class="frame-timestamp">
                      {galleryImages[selectedImageIndex]?.fileName ||
                        "Frame " + (selectedImageIndex + 1)}
                    </p>
                    {#if galleryImages[selectedImageIndex]?.detectionData}
                      <p class="frame-detections">
                        {galleryImages[selectedImageIndex].detectionData?.boxes
                          ?.length || 0} objects detected
                      </p>
                    {/if}
                  </div>
                </div>
                <!-- Thumbnail Strip -->
                <div class="gallery-thumbnails-strip">
                  {#each galleryImages as image, index}
                    <button
                      class="gallery-thumbnail"
                      class:selected={index === selectedImageIndex}
                      on:click={() => (selectedImageIndex = index)}
                    >
                      <img
                        src={image.annotated || image.original}
                        alt="Frame {index + 1}"
                      />
                      {#if image.detectionData}
                        <span class="thumbnail-badge"
                          >{image.detectionData.boxes?.length || 0}</span
                        >
                      {/if}
                    </button>
                  {/each}
                </div>
              {:else}
                <div class="empty-gallery">
                  <p>No captured frames yet</p>
                  <p class="text-muted">Start detection to capture frames</p>
                </div>
              {/if}
              {#if galleryImages.length > 0}
                <button
                  class="btn btn-clear-gallery"
                  on:click={() => {
                    galleryImages = [];
                    selectedImageIndex = 0;
                    currentGalleryIndex = 0;
                    processedFrameNumbers.clear();
                  }}
                  title="Clear all captured frames"
                >
                  <span class="clear-icon">🗑️</span>
                  <span class="clear-text">Clear Gallery</span>
                  <span class="clear-count">({galleryImages.length})</span>
                </button>
              {/if}
            </div>
          {:else if sourceType === "webcam" && webcamViewMode === "gallery" && !$useNewComponents}
            <!-- LEGACY: Webcam Gallery View -->
            <div class="gallery-view-container">
              {#if galleryImages.length > 0}
                <!-- Main Preview -->
                <div class="gallery-main-preview">
                  <img
                    src={galleryImages[selectedImageIndex]?.annotated ||
                      galleryImages[selectedImageIndex]?.original}
                    alt="Selected frame"
                    class="gallery-main-image"
                  />
                  <div class="gallery-main-info">
                    <p class="frame-timestamp">
                      {galleryImages[selectedImageIndex]?.timestamp ||
                        "Frame " + (selectedImageIndex + 1)}
                    </p>
                    {#if galleryImages[selectedImageIndex]?.detectionData}
                      <p class="frame-detections">
                        {galleryImages[selectedImageIndex].detectionData?.boxes
                          ?.length ||
                          galleryImages[selectedImageIndex].detectionData?.masks
                            ?.length ||
                          0} objects detected
                      </p>
                    {/if}
                  </div>
                </div>
                <!-- Thumbnail Strip -->
                <div class="gallery-thumbnails-strip">
                  {#each galleryImages as image, index}
                    <button
                      class="gallery-thumbnail"
                      class:selected={index === selectedImageIndex}
                      on:click={() => (selectedImageIndex = index)}
                    >
                      <img
                        src={image.annotated || image.original}
                        alt="Frame {index + 1}"
                      />
                      {#if image.detectionData}
                        <span class="thumbnail-badge"
                          >{image.detectionData.masks?.length ||
                            image.detectionData.boxes?.length ||
                            0}</span
                        >
                      {/if}
                    </button>
                  {/each}
                </div>
              {:else}
                <div class="empty-gallery">
                  <p>No captured frames yet</p>
                  <p class="text-muted">Start detection to capture frames</p>
                </div>
              {/if}
              {#if galleryImages.length > 0 && sourceType === "webcam"}
                <button
                  class="btn btn-clear-gallery"
                  on:click={() => {
                    galleryImages = [];
                    selectedImageIndex = 0;
                    currentGalleryIndex = 0;
                  }}
                  title="Clear all captured frames"
                >
                  <span class="clear-icon">🗑️</span>
                  <span class="clear-text">Clear Gallery</span>
                  <span class="clear-count">({galleryImages.length})</span>
                </button>
              {/if}
            </div>
          {:else}
            <div class="media-placeholder">
              <div class="placeholder-icon">📷</div>
              <p>Select a source to begin inference</p>
            </div>
          {/if}
        </div>
      {/if}

      <!-- Navigation arrows for gallery (image/batch mode) -->
      {#if galleryImages.length > 1 && (sourceType === "image" || sourceType === "batch")}
        <button
          class="gallery-nav gallery-nav-prev"
          on:click={() =>
            (currentGalleryIndex = Math.max(0, currentGalleryIndex - 1))}
          disabled={currentGalleryIndex === 0}
        >
          ‹
        </button>
        <button
          class="gallery-nav gallery-nav-next"
          on:click={() =>
            (currentGalleryIndex = Math.min(
              galleryImages.length - 1,
              currentGalleryIndex + 1,
            ))}
          disabled={currentGalleryIndex === galleryImages.length - 1}
        >
          ›
        </button>
      {/if}

      <!-- Image Gallery Controls (thumbnails and indicators) -->
      {#if $useNewModes && sourceType === "webcam" && webcamViewMode === "gallery" && galleryImages.length > 0}
        <!-- NEW: CaptureGallery Component for Webcam Thumbnails -->
        <CaptureGallery
          images={galleryImages}
          bind:selectedIndex={currentGalleryIndex}
          showMainPreview={false}
          onClear={handleComponentGalleryClear}
        />
      {:else if $useNewModes && sourceType === "rtsp" && rtspViewMode === "gallery" && galleryImages.length > 0}
        <!-- NEW: CaptureGallery Component for RTSP Thumbnails -->
        <CaptureGallery
          images={galleryImages}
          bind:selectedIndex={currentGalleryIndex}
          showMainPreview={false}
          onClear={handleComponentGalleryClear}
        />
      {:else if galleryImages.length > 1 && (sourceType === "image" || sourceType === "batch" || sourceType === "video")}
        <div class="gallery-controls">
          <div class="gallery-thumbnails">
            {#each galleryImages as image, idx}
              <button
                class="gallery-thumbnail {idx === currentGalleryIndex
                  ? 'active'
                  : ''}"
                on:click={() => (currentGalleryIndex = idx)}
                title={image.timestamp !== undefined
                  ? `Time: ${image.timestamp.toFixed(1)}s`
                  : image.fileName}
              >
                <img src={image.annotated} alt="Thumbnail {idx + 1}" />
                {#if image.timestamp !== undefined}
                  <span class="thumbnail-timestamp"
                    >{image.timestamp.toFixed(1)}s</span
                  >
                {/if}
              </button>
            {/each}
          </div>

          <div class="gallery-indicators">
            {#each galleryImages as _, idx}
              <button
                class="indicator {idx === currentGalleryIndex ? 'active' : ''}"
                on:click={() => (currentGalleryIndex = idx)}
                aria-label="Go to image {idx + 1}"
              ></button>
            {/each}
          </div>
        </div>
      {/if}

      <!-- Detection Results -->
      {#if $useNewComponents}
        <!-- NEW: StatsPanel Component -->
        <StatsPanel
          {detectionResults}
          showFrameStats={sourceType === "video" ||
            sourceType === "webcam" ||
            sourceType === "rtsp"}
          frameWidth={frameStats.width}
          frameHeight={frameStats.height}
          fps={frameStats.fps}
        />
      {:else}
        <!-- LEGACY: Inline Results Area -->
        <div class="results-area">
          <h3>Results</h3>
          <div class="results-list">
            {#if detectionResults.length > 0}
              {#each detectionResults as result}
                <span class="result-badge">
                  {result.class_name} ({(result.confidence * 100).toFixed(0)}%)
                </span>
              {/each}
            {:else}
              <p class="text-muted">No results yet</p>
            {/if}
          </div>
          {#if detectionResults.length > 0}
            <div class="results-summary">
              Total: {detectionResults.length} objects
            </div>
          {/if}
        </div>
      {/if}
    </div>

    <!-- Right Sidebar -->
    <aside class="control-sidebar">
      <!-- Detection Controls -->
      <section class="control-section">
        <h3>Controls</h3>

        {#if $useNewComponents}
          <!-- NEW: ModelSelector Component -->
          <ModelSelector
            {models}
            bind:selectedModelId
            {selectedModel}
            disabled={isDetecting}
            showTaskInstructions={true}
            onModelChange={handleComponentModelChange}
          />
        {:else}
          <!-- LEGACY: Inline Model Selector -->
          <div class="model-selector">
            <label>
              <span>Model: <span class="required">*</span></span>
              <select
                bind:value={selectedModelId}
                on:change={handleModelSelect}
                disabled={isDetecting}
              >
                <option value="">Choose Model</option>
                {#each models as model}
                  <option value={model.id}
                    >{model.name} ({model.base_type})</option
                  >
                {/each}
              </select>
            </label>
          </div>

          <!-- Task-Specific Instructions -->
          {#if selectedModel && selectedModel.task_type}
            <div class="task-instructions">
              <div
                class="task-badge-header"
                on:click={() => (isTipsCollapsed = !isTipsCollapsed)}
              >
                {#if selectedModel.task_type === "detect"}
                  <span class="task-badge detect">🎯 Detection Model</span>
                {:else if selectedModel.task_type === "classify"}
                  <span class="task-badge classify"
                    >🏷️ Classification Model</span
                  >
                {:else if selectedModel.task_type === "segment"}
                  <span class="task-badge segment">✂️ Segmentation Model</span>
                {/if}
                <button
                  class="collapse-toggle"
                  type="button"
                  aria-label="Toggle tips"
                >
                  {isTipsCollapsed ? "▼" : "▲"}
                </button>
              </div>

              {#if !isTipsCollapsed}
                <div class="tips-content">
                  <p class="task-description">
                    {getTaskInstructions(selectedModel.task_type)}
                  </p>

                  {#if getTaskFormatGuidance(selectedModel.task_type).items.length > 0}
                    {@const guidance = getTaskFormatGuidance(
                      selectedModel.task_type,
                    )}
                    <div class="task-guidance">
                      <strong>{guidance.title}</strong>
                      <ul>
                        {#each guidance.items as item}
                          <li>{item}</li>
                        {/each}
                      </ul>
                    </div>
                  {/if}

                  <!-- Task Mismatch Warning -->
                  {#if getTaskMismatchWarning(selectedModel.task_type, sourceType)}
                    <div class="warning-box">
                      {getTaskMismatchWarning(
                        selectedModel.task_type,
                        sourceType,
                      )}
                    </div>
                  {/if}
                </div>
              {/if}
            </div>
          {/if}
        {/if}

        {#if $useNewComponents}
          <!-- NEW: SourceTypeSelector Component -->
          <SourceTypeSelector bind:sourceType disabled={isDetecting} />
        {:else}
          <!-- LEGACY: Inline Source Selector -->
          <div class="source-selector">
            <label for="sourceType">
              <span>Source Type:</span>
              <select
                name="sourceType"
                bind:value={sourceType}
                disabled={isDetecting}
              >
                <option value="image">Single Image</option>
                <option value="batch">Batch Images</option>
                <option value="video">Video File</option>
                <option value="webcam">Camera (Web)</option>
                <option value="rtsp">RTSP Stream</option>
              </select>
            </label>
          </div>
        {/if}

        <!-- Model Prompts UI (for models requiring prompts on video/webcam/rtsp sources) -->
        {#if promptRequired && (sourceType === "video" || sourceType === "webcam" || sourceType === "rtsp") && selectedModelId}
          <div class="prompts-controls">
            <h4
              style="margin: 0 0 0.75rem 0; font-size: 0.9rem; color: var(--color-text-secondary);"
            >
              🎭 Prompts
            </h4>

            <!-- Prompt Validation Banner -->
            {#if inferPrompts.length === 0}
              <div class="warning-box" style="margin-bottom: 0.75rem;">
                ⚠️ This model requires at least one prompt. Add a text, point,
                or box prompt below.
              </div>
            {/if}

            <!-- Prompt Mode Selector -->
            <div class="prompt-mode-selector">
              <button
                class="prompt-mode-btn"
                class:active={promptMode === "text"}
                on:click={() => (promptMode = "text")}
                disabled={isDetecting}
                type="button"
              >
                📝 Text
              </button>
              <button
                class="prompt-mode-btn"
                class:active={promptMode === "point"}
                on:click={() => (promptMode = "point")}
                disabled={isDetecting || sourceType !== "video"}
                type="button"
              >
                📍 Point
              </button>
              <button
                class="prompt-mode-btn"
                class:active={promptMode === "box"}
                on:click={() => (promptMode = "box")}
                disabled={isDetecting || sourceType !== "video"}
                type="button"
              >
                ▢ Box
              </button>
            </div>

            <!-- Hint Text -->
            <p class="prompt-hint">
              {#if promptMode === "text"}
                Enter natural language descriptions of objects to segment
              {:else if promptMode === "point"}
                Click on video frame to add foreground points (Shift+Click for
                background)
              {:else if promptMode === "box"}
                Click and drag on video frame to draw bounding boxes
              {/if}
            </p>

            <!-- Text Prompt Input -->
            {#if promptMode === "text"}
              <div class="text-prompt-input">
                <input
                  type="text"
                  bind:value={textPrompt}
                  placeholder="e.g., 'white bicycle', 'person wearing red shirt'"
                  on:keydown={(e) => {
                    if (e.key === "Enter") addtextPrompt();
                  }}
                  disabled={isDetecting}
                />
                <button
                  class="btn btn-primary btn-sm"
                  on:click={addtextPrompt}
                  disabled={!textPrompt.trim() || isDetecting}
                  type="button"
                >
                  Add
                </button>
              </div>
            {/if}

            <!-- Prompts List -->
            {#if inferPrompts.length > 0}
              <div class="prompts-list">
                <div class="prompts-header">
                  <span>Prompts ({inferPrompts.length})</span>
                  <button
                    class="btn-clear"
                    on:click={clearInferencePrompts}
                    disabled={isDetecting}
                    type="button"
                  >
                    Clear All
                  </button>
                </div>
                <div class="prompts-items">
                  {#each inferPrompts as prompt, idx}
                    <div class="prompt-item">
                      <span class="prompt-icon">
                        {#if prompt.type === "text"}
                          📝
                        {:else if prompt.type === "point"}
                          {prompt.label === 1 ? "✅" : "❌"}
                        {:else if prompt.type === "box"}
                          ▢
                        {/if}
                      </span>
                      <span class="prompt-label">
                        {#if prompt.type === "text"}
                          {prompt.value}
                        {:else if prompt.type === "point"}
                          Point ({prompt.coords?.[0] || 0}, {prompt
                            .coords?.[1] || 0})
                          {prompt.label === 1 ? "(fg)" : "(bg)"}
                        {:else if prompt.type === "box"}
                          Box ({prompt.coords?.[0] || 0}, {prompt.coords?.[1] ||
                            0}) → ({prompt.coords?.[2] || 0}, {prompt
                            .coords?.[3] || 0})
                        {/if}
                      </span>
                      <button
                        class="btn-remove"
                        on:click={() => removeInferencePrompt(idx)}
                        disabled={isDetecting}
                        type="button"
                      >
                        ✕
                      </button>
                    </div>
                  {/each}
                </div>
              </div>
            {/if}
          </div>
        {/if}

        {#if sourceType === "rtsp"}
          <div class="form-group">
            <label>
              <span>RTSP URL:</span>
              <input
                type="text"
                bind:value={rtspUrl}
                placeholder="rtsp://..."
                disabled={isDetecting}
              />
            </label>
          </div>
          <!-- RTSP Capture Mode Toggle -->
          <div class="form-group">
            <div style="margin-bottom: 0.5rem; display: block;">
              <span
                style="font-size: 0.875rem; color: var(--color-text-secondary);"
                >Capture Mode:</span
              >
            </div>
            <div class="segmented-control" style="margin-bottom: 0.5rem;">
              <button
                class="segment"
                class:active={rtspCaptureMode === "manual"}
                on:click={() => (rtspCaptureMode = "manual")}
                disabled={isDetecting}
              >
                📸 Manual Capture
              </button>
              <button
                class="segment"
                class:active={rtspCaptureMode === "continuous"}
                on:click={() => (rtspCaptureMode = "continuous")}
                disabled={isDetecting}
              >
                📹 Continuous Recording
              </button>
            </div>
            {#if rtspCaptureMode === "continuous"}
              <p
                class="warning-text"
                style="font-size: 0.8rem; color: #ff9500; margin: 0.5rem 0 0 0;"
              >
                ⚠️ Continuous mode stores unlimited frames. Storage costs may
                apply.
              </p>
            {/if}
          </div>
        {:else if sourceType === "webcam"}
          <!-- Capture Mode Toggle -->
          <div class="form-group">
            <div style="margin-bottom: 0.5rem; display: block;">
              <span
                style="font-size: 0.875rem; color: var(--color-text-secondary);"
                >Capture Mode:</span
              >
            </div>
            <div class="segmented-control" style="margin-bottom: 0.5rem;">
              <button
                class="segment"
                class:active={webcamCaptureMode === "manual"}
                on:click={() => (webcamCaptureMode = "manual")}
                disabled={isDetecting}
              >
                📸 Manual Capture
              </button>
              <button
                class="segment"
                class:active={webcamCaptureMode === "continuous"}
                on:click={() => (webcamCaptureMode = "continuous")}
                disabled={isDetecting}
              >
                📹 Continuous Recording
              </button>
            </div>
          </div>
        {:else}
          {#if $useNewComponents}
            <!-- NEW: FileUploadControl Component -->
            <FileUploadControl
              {sourceType}
              bind:selectedFile
              bind:selectedFiles
              {imagePreview}
              disabled={isDetecting}
              onFileSelect={handleComponentFileSelect}
              onFilesSelect={handleComponentFilesSelect}
              onClearPreview={handleComponentClearPreview}
            />
          {:else}
            <!-- LEGACY: Inline File Upload -->
            <div class="form-group">
              <label class="file-input-label">
                <span class="btn btn-secondary btn-sm">
                  {sourceType === "batch" ? "Select Files" : "Select File"}
                </span>
                <input
                  bind:this={fileInputElement}
                  type="file"
                  on:change={handleFileUpload}
                  accept={sourceType === "image" || sourceType === "batch"
                    ? "image/*"
                    : sourceType === "video"
                      ? "video/*"
                      : "*"}
                  multiple={sourceType === "batch"}
                  disabled={isDetecting}
                  style="display: none;"
                />
              </label>
              {#if selectedFile}
                <p class="file-name">{selectedFile.name}</p>
              {:else if selectedFiles.length > 0}
                <p class="file-name">
                  {#if selectedFiles.length <= 3}
                    {selectedFiles.map((f) => f.name).join(", ")}
                  {:else}
                    {selectedFiles
                      .slice(0, 3)
                      .map((f) => f.name)
                      .join(", ")} +{selectedFiles.length - 3} more... of total {selectedFiles.length}
                    images
                  {/if}
                </p>
              {/if}
            </div>
          {/if}

          <!-- Video Capture Mode Toggle -->
          {#if sourceType === "video" && selectedFile}
            <div class="form-group">
              <div style="margin-bottom: 0.5rem; display: block;">
                <span
                  style="font-size: 0.875rem; color: var(--color-text-secondary);"
                  >Detection Mode:</span
                >
              </div>
              <div class="segmented-control" style="margin-bottom: 0.5rem;">
                <button
                  class="segment"
                  class:active={videoCaptureMode === "manual"}
                  on:click={() => (videoCaptureMode = "manual")}
                  disabled={isDetecting}
                >
                  📸 Manual Capture
                </button>
                <button
                  class="segment"
                  class:active={videoCaptureMode === "continuous"}
                  on:click={() => (videoCaptureMode = "continuous")}
                  disabled={isDetecting}
                >
                  🎬 Detect All Frames
                </button>
              </div>
              {#if videoCaptureMode === "manual"}
                <p
                  class="help-text"
                  style="font-size: 0.8rem; margin: 0.5rem 0 0 0;"
                >
                  Use video controls to navigate, then capture frames with Space
                  bar
                </p>
              {:else}
                <p
                  class="help-text"
                  style="font-size: 0.8rem; margin: 0.5rem 0 0 0;"
                >
                  Processes video in background using Skip Frames setting
                </p>
              {/if}
            </div>
          {/if}
        {/if}

        <!-- Batch Progress Indicator -->
        {#if promptRequired && sourceType === "batch" && isDetecting && batchProcessingProgress.total > 0}
          <div class="batch-progress-indicator">
            <div class="progress-header">
              <span>Processing...</span>
              <span class="progress-count"
                >{batchProcessingProgress.current} / {batchProcessingProgress.total}</span
              >
            </div>
            <div class="progress-bar">
              <div
                class="progress-fill"
                style="width: {(batchProcessingProgress.current /
                  batchProcessingProgress.total) *
                  100}%"
              ></div>
            </div>
            <p class="progress-file">{batchProcessingProgress.fileName}</p>
          </div>
        {/if}

        {#if $useNewComponents}
          <!-- NEW: CaptureControls Component -->
          <CaptureControls
            {sourceType}
            captureMode={sourceType === "video"
              ? videoCaptureMode
              : sourceType === "rtsp"
                ? rtspCaptureMode
                : sourceType === "webcam"
                  ? webcamCaptureMode
                  : "manual"}
            {isDetecting}
            hasFile={sourceType === "batch"
              ? selectedFiles.length > 0
              : selectedFile !== null}
            hasUrl={rtspUrl.trim() !== ""}
            onStart={startPrediction}
            onStop={stopRtspInference}
            onCapture={() => {
              if (sourceType === "rtsp") {
                if (promptRequired && rtsp_viewer) {
                  rtsp_viewer.captureCurrentFrame();
                } else {
                  captureCurrentRTSPFrame();
                }
              } else if (sourceType === "video") {
                captureCurrentVideoFrame();
              } else if (sourceType === "webcam") {
                captureCurrentCameraFrame();
              }
            }}
          />
        {:else}
          <!-- LEGACY: Inline Capture Controls -->
          <button
            class="btn btn-primary btn-start btn-start-large"
            on:click={isDetecting &&
            (sourceType === "webcam" ||
              sourceType === "rtsp" ||
              (sourceType === "video" && videoCaptureMode === "manual"))
              ? stopRtspInference
              : startPrediction}
            disabled={!selectedModelId ||
              (promptRequired &&
                (sourceType === "video" ||
                  sourceType === "webcam" ||
                  sourceType === "rtsp") &&
                inferPrompts.length === 0)}
          >
            {isDetecting &&
            (sourceType === "webcam" ||
              sourceType === "rtsp" ||
              (sourceType === "video" && videoCaptureMode === "manual"))
              ? "⏹ Stop"
              : isDetecting
                ? "⏳ Detecting..."
                : "▶ Start"}
          </button>

          {#if sourceType === "rtsp" && isDetecting && rtspCaptureMode === "manual"}
            <button
              class="btn btn-accent btn-capture"
              on:click={() => {
                if (promptRequired && rtsp_viewer) {
                  rtsp_viewer.captureCurrentFrame();
                } else {
                  captureCurrentRTSPFrame();
                }
              }}
              disabled={!isDetecting}
              title="Capture current frame (Space or Enter)"
            >
              📸 Capture Frame <span style="opacity: 0.7; font-size: 0.875rem;"
                >(Space)</span
              >
            </button>
            <p class="capture-counter">
              Captured: <strong>{galleryImages.length}</strong>
              frame{galleryImages.length !== 1 ? "s" : ""}
            </p>
          {/if}

          {#if sourceType === "video" && isDetecting && videoCaptureMode === "manual"}
            <!-- Active Session Banner -->
            {#if activeVideoSession}
              <div class="session-banner">
                <div class="session-info">
                  <span class="session-icon">📹</span>
                  <div class="session-details">
                    <strong>Active Session:</strong>
                    <span class="session-filename"
                      >{selectedFile?.name || "Video"}</span
                    >
                  </div>
                  <div class="session-stats">
                    <span class="capture-count"
                      >{galleryImages.length} frames captured</span
                    >
                  </div>
                </div>
                <button
                  class="btn btn-sm btn-outline"
                  on:click={finishVideoSession}
                >
                  Finish Session
                </button>
              </div>
            {/if}

            <button
              class="btn btn-accent btn-capture"
              on:click={captureCurrentVideoFrame}
              disabled={!activeVideoSession ||
                !videoElement ||
                (promptRequired && inferPrompts.length === 0)}
              title="Capture current frame (Space or Enter)"
            >
              📸 Capture Frame <span style="opacity: 0.7; font-size: 0.875rem;"
                >(Space)</span
              >
            </button>
            <p class="capture-counter">
              Captured: <strong>{galleryImages.length}</strong>
              frame{galleryImages.length !== 1 ? "s" : ""}
            </p>
          {/if}

          {#if sourceType === "webcam" && isDetecting && webcamCaptureMode === "manual"}
            <button
              class="btn btn-accent btn-capture"
              on:click={captureCurrentCameraFrame}
              disabled={!lastPredictionResponse}
              title="Capture current frame (Space or Enter)"
            >
              📸 Capture Frame <span style="opacity: 0.7; font-size: 0.875rem;"
                >(Space)</span
              >
            </button>
            <p class="capture-counter">
              Captured: <strong>{galleryImages.length}</strong>
              frame{galleryImages.length !== 1 ? "s" : ""}
            </p>
          {/if}

          <button
            class="btn btn-outline btn-reset"
            on:click={resetDetection}
            disabled={isDetecting}
          >
            Reset
          </button>
        {/if}
      </section>

      <!-- Smart Settings -->
      <section class="control-section">
        <h3>Smart Settings</h3>

        {#if $useNewComponents}
          <!-- NEW: SmartSettingsPanel Component -->
          <SmartSettingsPanel
            bind:confidence
            bind:classFilter
            disabled={isDetecting}
            bind:skipFrames
            showSkipFrames={sourceType === "video" || sourceType === "rtsp"}
          />
        {:else}
          <!-- LEGACY: Inline Smart Settings -->
          <div class="confidence-control">
            <label>
              <span>Confidence: {confidence.toFixed(2)}</span>
              <input
                type="range"
                min="0"
                max="1"
                step="0.05"
                bind:value={confidence}
                disabled={isDetecting}
              />
            </label>
            <p class="help-text">Lower = more detections</p>
          </div>

          <!-- Class Filter -->
          <div class="class-filter-control">
            <div class="section-header">
              <span>Class Filter (optional)</span>
            </div>
            <div class="tags-input-container">
              <div class="tags-list">
                {#each classFilter as tag}
                  <span class="tag">
                    {tag}
                    <button
                      type="button"
                      class="tag-remove"
                      disabled={isDetecting}
                      on:click={() => {
                        classFilter = classFilter.filter((t) => t !== tag);
                      }}
                    >
                      ×
                    </button>
                  </span>
                {/each}
                <input
                  type="text"
                  class="tag-input"
                  placeholder="Add a class..."
                  disabled={isDetecting}
                  bind:value={newClassTag}
                  on:keydown={(e) => {
                    if (e.key === "Enter" && newClassTag.trim()) {
                      e.preventDefault();
                      const trimmedTag = newClassTag.trim();
                      if (!classFilter.includes(trimmedTag)) {
                        classFilter = [...classFilter, trimmedTag];
                      }
                      newClassTag = "";
                    }
                  }}
                />
              </div>
            </div>
            <p class="help-text">
              Only predict specific classes. Press Enter to add (leave empty for
              all)
            </p>
          </div>

          {#if sourceType === "video" || sourceType === "rtsp"}
            <div class="skip-frames-control">
              <label>
                <span
                  >Skip Frames: {skipFrames}
                  {skipFrames === 1
                    ? "(All frames)"
                    : `(Every ${skipFrames}${skipFrames === 2 ? "nd" : skipFrames === 3 ? "rd" : "th"} frame)`}</span
                >
                <input
                  type="range"
                  min="1"
                  max="30"
                  step="1"
                  bind:value={skipFrames}
                  disabled={isDetecting}
                />
              </label>
              <p class="help-text">
                {#if promptRequired && skipFrames < 10}
                  <span class="warning-badge">⚠️ Performance Warning</span>
                  <br />
                  Skip frames &lt; 10 may cause lag due to this Model's slower inference
                  (200-800ms per frame). Recommended: 10+ frames for smooth streaming.
                {:else if skipFrames === 1}
                  ⚠️ Processing all frames - High CPU/storage usage
                {:else if skipFrames <= 3}
                  ⚠️ Low skip value - Moderate processing load
                {:else}
                  Higher = faster processing, fewer detections
                {/if}
              </p>
              {#if promptRequired}
                <p
                  class="info-text"
                  style="font-size: 0.8rem; color: var(--color-text-secondary); margin-top: 4px;"
                >
                  💡 Model inference: ~200-800ms/frame. Lower skip values
                  increase queue buildup risk.
                </p>
              {/if}
            </div>
          {/if}
        {/if}

        <!-- Prompt Controls (for image and batch modes only) -->
        {#if promptRequired && sourceType !== "video" && sourceType !== "webcam" && sourceType !== "rtsp"}
          <div class="segment-controls">
            <div class="section-header">
              <span>Segmentation Mode</span>
            </div>

            <!-- Prompt Mode Selector -->
            <div class="prompt-mode-selector">
              <button
                class="prompt-mode-btn {promptMode === 'auto' ? 'active' : ''}"
                on:click={() => (promptMode = "auto")}
                disabled={isDetecting}
                type="button"
              >
                🤖 Auto
              </button>
              <button
                class="prompt-mode-btn {promptMode === 'text' ? 'active' : ''}"
                on:click={() => (promptMode = "text")}
                disabled={isDetecting}
                type="button"
              >
                ✍️ Text
              </button>
              <button
                class="prompt-mode-btn {promptMode === 'point' ? 'active' : ''}"
                on:click={() => (promptMode = "point")}
                disabled={isDetecting}
                type="button"
              >
                📍 Point
              </button>
              <button
                class="prompt-mode-btn {promptMode === 'box' ? 'active' : ''}"
                on:click={() => (promptMode = "box")}
                disabled={isDetecting}
                type="button"
              >
                ⬜ Box
              </button>
            </div>

            <!-- Mode-specific help text -->
            <p class="help-text prompt-hint">
              {#if promptMode === "auto"}
                🤖 No prompts needed - automatically segments all objects
              {:else if promptMode === "text"}
                ✍️ Describe what to segment (e.g., "person", "car", "building")
              {:else if promptMode === "point"}
                📍 Click on canvas to add points • Shift+Click for background
              {:else if promptMode === "box"}
                ⬜ Click and drag on canvas to draw bounding boxes
              {/if}
            </p>

            <!-- Text Prompt Input -->
            {#if promptMode === "text"}
              <div class="text-prompt-input">
                <input
                  type="text"
                  placeholder="e.g., person, car, tree"
                  bind:value={textPrompt}
                  disabled={isDetecting}
                  on:keydown={(e) => {
                    if (e.key === "Enter" && textPrompt.trim()) {
                      e.preventDefault();
                      addtextPrompt();
                    }
                  }}
                />
                <button
                  class="btn btn-sm btn-accent"
                  on:click={addtextPrompt}
                  disabled={isDetecting || !textPrompt.trim()}
                  type="button"
                >
                  Add
                </button>
              </div>
            {/if}

            <!-- Prompts List -->
            {#if inferPrompts.length > 0}
              <div class="prompts-list">
                <div class="prompts-header">
                  <span>Added Prompts ({inferPrompts.length})</span>
                  <button
                    class="btn-clear"
                    on:click={clearInferencePrompts}
                    disabled={isDetecting}
                    type="button"
                    title="Clear all prompts"
                  >
                    Clear All
                  </button>
                </div>
                <div class="prompts-items">
                  {#each inferPrompts as prompt, index}
                    <div class="prompt-item">
                      {#if prompt.type === "text"}
                        <span class="prompt-icon">✍️</span>
                        <span class="prompt-label">"{prompt.value}"</span>
                      {:else if prompt.type === "point"}
                        <span
                          class="prompt-icon"
                          style="color: {prompt.label === 1
                            ? '#22c55e'
                            : '#ef4444'}"
                        >
                          {prompt.label === 1 ? "●" : "●"}
                        </span>
                        <span class="prompt-label">
                          {prompt.label === 1 ? "Foreground" : "Background"} ({prompt
                            .coords?.[0] || 0},
                          {prompt.coords?.[1] || 0})
                        </span>
                      {:else if prompt.type === "box"}
                        <span class="prompt-icon">⬜</span>
                        <span class="prompt-label">
                          Box ({prompt.coords?.[0] || 0}, {prompt.coords?.[1] ||
                            0}) → ({prompt.coords?.[2] || 0}, {prompt
                            .coords?.[3] || 0})
                        </span>
                      {/if}
                      <button
                        class="btn-remove"
                        on:click={() => removeInferencePrompt(index)}
                        disabled={isDetecting}
                        type="button"
                        title="Remove prompt"
                      >
                        ×
                      </button>
                    </div>
                  {/each}
                </div>
              </div>
            {/if}
          </div>
        {/if}

        {#if galleryImages.length > 0 && (sourceType === "image" || sourceType === "batch" || sourceType === "video" || ($useNewModes && ((sourceType === "webcam" && webcamViewMode === "gallery") || (sourceType === "rtsp" && rtspViewMode === "gallery"))))}
          <div class="zoom-control">
            <label>
              <span>Zoom: {zoomLevel.toFixed(1)}x</span>
              <input
                type="range"
                min="1"
                max="5"
                step="0.1"
                bind:value={zoomLevel}
                on:input={() => {
                  if (zoomLevel === 1) {
                    panOffset = { x: 0, y: 0 };
                  }
                }}
              />
            </label>
            <p class="help-text">Scroll to zoom image and bounding boxes</p>
            {#if zoomLevel > 1}
              <button
                class="btn btn-sm btn-outline"
                on:click={() => {
                  zoomLevel = 1;
                  panOffset = { x: 0, y: 0 };
                }}
              >
                Reset Zoom
              </button>
            {/if}
          </div>
        {/if}

        {#if availableClasses.length > 0}
          <div class="class-filters">
            <span class="filter-label">Classes:</span>
            <div class="filter-buttons">
              <button
                class="filter-btn {selectedClasses.size === 0 ? 'active' : ''}"
                on:click={() => {
                  selectedClasses.clear();
                  selectedClasses = selectedClasses;
                  drawDetections();
                }}
              >
                All
              </button>
              {#each availableClasses as className}
                <button
                  class="filter-btn {selectedClasses.has(className)
                    ? 'active'
                    : ''}"
                  on:click={() => toggleClassFilter(className)}
                >
                  {className}
                </button>
              {/each}
            </div>
          </div>
        {/if}
      </section>

      <!-- Current Frame Stats -->
      {#if Object.keys(classCounts).length > 0}
        <section class="control-section">
          <h3>Current Frame</h3>
          <div class="frame-stats">
            {#each Object.entries(classCounts) as [className, count]}
              <div class="stat-row">
                <span class="stat-label">{className}:</span>
                <span class="stat-value">{count}</span>
              </div>
            {/each}
            {#if Object.keys(classCounts).length === 0}
              <p class="text-muted">No detections</p>
            {/if}
          </div>
        </section>
      {/if}

      <!-- Job Status (if batch/video/rtsp) -->
      {#if currentJob}
        <section class="control-section">
          <h3>Job Status</h3>
          <div class="job-status">
            <div class="status-badge badge-{currentJob.status}">
              {currentJob.status.toUpperCase()}
            </div>
            {#if currentJob.progress !== undefined}
              <div class="progress-bar">
                <div
                  class="progress-fill"
                  style="width: {currentJob.progress}%"
                ></div>
              </div>
              {#if currentJob.mode === "webcam" || currentJob.mode === "rtsp"}
                <p class="progress-text">
                  {currentJob.progress} frames processed
                </p>
              {:else}
                <p class="progress-text">{currentJob.progress}%</p>
              {/if}
            {/if}
          </div>
        </section>
      {/if}
    </aside>
  </div>
</div>

<!-- Inactivity Warning Modal -->
{#if showInactivityModal}
  <div
    class="modal-overlay"
    role="dialog"
    aria-modal="true"
    on:click={() => (showInactivityModal = false)}
  >
    <div
      class="modal-content inactivity-modal"
      role="document"
      on:click|stopPropagation
    >
      <div class="modal-icon">⚠️</div>
      <h2>Session Inactive</h2>
      <p>Your video analysis session has been idle for 10 minutes.</p>
      <p class="modal-subtitle">
        Would you like to continue or finish the session?
      </p>

      <div class="modal-actions">
        <button class="btn btn-outline" on:click={continueSession}>
          Continue Session
        </button>
        <button
          class="btn btn-primary"
          on:click={async () => {
            showInactivityModal = false;
            await finishVideoSession();
          }}
        >
          Finish Session
        </button>
      </div>
    </div>
  </div>
{/if}

<style>
  .detection-page {
    height: 90vh;
    display: flex;
    flex-direction: column;
    background-color: var(--color-navy-dark);
    color: var(--color-white);
  }

  /* Header */
  .detection-header {
    display: flex;
    align-items: center;
    justify-content: space-between;
    padding: var(--spacing-md) var(--spacing-lg);
    background-color: var(--color-navy);
    border-bottom: 2px solid rgba(225, 96, 76, 0.3);
  }

  .header-left h1 {
    font-size: var(--font-size-xl);
    margin: 0;
    color: var(--color-white);
    font-weight: 700;
  }

  /* Main Content */
  .detection-content {
    flex: 1;
    display: flex;
    gap: var(--spacing-lg);
    padding: var(--spacing-lg);
    overflow: hidden;
  }

  /* Media Area */
  .media-area {
    flex: 1;
    display: flex;
    flex-direction: column;
    gap: var(--spacing-md);
    min-width: 0;
  }

  .media-container {
    position: relative;
    flex: 1;
    background-color: rgba(0, 0, 0, 0.3);
    border-radius: var(--radius-md);
    overflow: hidden;
    display: flex;
    align-items: center;
    justify-content: center;
  }

  .media-container.zoomed {
    overflow: auto;
  }

  .media-placeholder {
    text-align: center;
    color: var(--color-text-light);
  }

  .placeholder-icon {
    font-size: 4rem;
    margin-bottom: var(--spacing-md);
    opacity: 0.5;
  }

  .detection-canvas {
    max-width: 100%;
    max-height: 100%;
    object-fit: contain;
  }

  .detection-canvas.segment-canvas {
    cursor: crosshair;
  }

  .detection-canvas.overlay {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    pointer-events: none;
  }

  .rtsp-stream-container {
    position: relative;
    width: 100%;
    height: 100%;
    display: flex;
    align-items: center;
    justify-content: center;
  }

  .rtsp-stream-container canvas {
    max-width: 100%;
    max-height: 100%;
    object-fit: contain;
  }

  .rtsp-frame-status-overlay {
    position: absolute;
    top: 50%;
    left: 50%;
    transform: translate(-50%, -50%);
    z-index: 10;
    background: rgba(29, 47, 67, 0.95);
    backdrop-filter: blur(10px);
    padding: 2rem 3rem;
    border-radius: 12px;
    box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);
    border: 1px solid rgba(255, 255, 255, 0.1);
  }

  .status-message {
    text-align: center;
    color: white;
  }

  .status-message.loading {
    display: flex;
    flex-direction: column;
    align-items: center;
    gap: 1rem;
  }

  .status-message p {
    margin: 0;
    font-size: 1.1rem;
    font-weight: 600;
    color: white;
  }

  .status-message small {
    font-size: 0.875rem;
    color: rgba(255, 255, 255, 0.7);
  }

  .spinner {
    width: 40px;
    height: 40px;
    border: 3px solid rgba(255, 255, 255, 0.2);
    border-top-color: var(--color-accent);
    border-radius: 50%;
    animation: spin 1s linear infinite;
  }

  @keyframes spin {
    to {
      transform: rotate(360deg);
    }
  }

  .video-display {
    max-width: 100%;
    max-height: 100%;
    width: 100%;
    height: 100%;
    object-fit: contain;
  }

  /* Webcam Mode Switcher (iPhone-style) */
  .webcam-mode-switcher {
    display: flex;
    justify-content: center;
    margin-bottom: 1rem;
    padding: 0.5rem 0;
  }

  .segmented-control {
    display: inline-flex;
    background: rgba(255, 255, 255, 0.1);
    border-radius: 10px;
    padding: 4px;
    gap: 4px;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.15);
    backdrop-filter: blur(10px);
  }

  .segment-button {
    display: flex;
    align-items: center;
    gap: 8px;
    padding: 10px 20px;
    border: none;
    background: transparent;
    color: rgba(255, 255, 255, 0.6);
    font-size: 0.95rem;
    font-weight: 500;
    border-radius: 8px;
    cursor: pointer;
    transition: all 0.2s ease;
    position: relative;
    min-width: 120px;
    justify-content: center;
  }

  .segment-button:disabled {
    opacity: 0.4;
    cursor: not-allowed;
  }

  .segment-button:not(:disabled):hover {
    color: rgba(255, 255, 255, 0.8);
  }

  .segment-button.active {
    background: rgba(255, 255, 255, 0.95);
    color: var(--color-navy);
    box-shadow: 0 2px 6px rgba(0, 0, 0, 0.2);
  }

  /* Capture Mode Toggle - Enhanced Active State */
  .segment {
    display: flex;
    align-items: center;
    gap: 6px;
    padding: 10px 18px;
    border: 2px solid transparent;
    background: rgba(255, 255, 255, 0.05);
    color: rgba(255, 255, 255, 0.5);
    font-size: 0.9rem;
    font-weight: 500;
    border-radius: 8px;
    cursor: pointer;
    transition: all 0.25s ease;
    position: relative;
    flex: 1;
    justify-content: center;
  }

  .segment:disabled {
    opacity: 0.3;
    cursor: not-allowed;
  }

  .segment:not(:disabled):hover {
    background: rgba(255, 255, 255, 0.1);
    color: rgba(255, 255, 255, 0.7);
    border-color: rgba(255, 255, 255, 0.2);
    transform: translateY(-1px);
  }

  .segment.active {
    background: linear-gradient(135deg, var(--color-accent), #d55540);
    color: white;
    font-weight: 600;
    border-color: var(--color-accent);
    box-shadow:
      0 4px 12px rgba(225, 96, 76, 0.4),
      0 0 0 3px rgba(225, 96, 76, 0.1);
    transform: scale(1.02);
  }

  .segment.active:hover {
    background: linear-gradient(135deg, #d55540, var(--color-accent));
    transform: scale(1.02);
  }

  .segment-icon {
    font-size: 1.1rem;
    line-height: 1;
  }

  .segment-label {
    font-weight: 600;
  }

  .rec-badge {
    position: absolute;
    top: 6px;
    right: 6px;
    background: #ff3b30;
    color: white;
    font-size: 0.65rem;
    font-weight: 700;
    padding: 2px 6px;
    border-radius: 8px;
    letter-spacing: 0.5px;
    animation: pulse 2s ease-in-out infinite;
  }

  .count-badge {
    background: var(--color-accent);
    color: white;
    font-size: 0.75rem;
    font-weight: 600;
    padding: 2px 8px;
    border-radius: 10px;
    min-width: 20px;
    text-align: center;
  }

  @keyframes pulse {
    0%,
    100% {
      opacity: 1;
    }
    50% {
      opacity: 0.6;
    }
  }

  /* Gallery View Container */
  .gallery-view-container {
    width: 100%;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 1rem;
    padding: 1rem;
    background: #000;
    border-radius: 8px;
  }

  .gallery-main-preview {
    flex: 1;
    position: relative;
    display: flex;
    flex-direction: column;
    background: #000;
    border-radius: 8px;
    overflow: hidden;
  }

  .gallery-main-image {
    width: 100%;
    height: 100%;
    object-fit: contain;
  }

  .gallery-main-info {
    position: absolute;
    bottom: 0;
    left: 0;
    right: 0;
    background: linear-gradient(to top, rgba(0, 0, 0, 0.8), transparent);
    padding: 1.5rem 1rem 1rem;
    color: white;
  }

  .frame-timestamp {
    font-size: 0.9rem;
    margin: 0 0 0.25rem 0;
    opacity: 0.9;
  }

  .frame-detections {
    font-size: 0.85rem;
    margin: 0;
    opacity: 0.7;
  }

  .gallery-thumbnails-strip {
    display: flex;
    gap: 0.5rem;
    overflow-x: auto;
    padding: 0.5rem 0;
    scrollbar-width: thin;
    scrollbar-color: rgba(255, 255, 255, 0.3) transparent;
  }

  .gallery-thumbnails-strip::-webkit-scrollbar {
    height: 6px;
  }

  .gallery-thumbnails-strip::-webkit-scrollbar-track {
    background: transparent;
  }

  .gallery-thumbnails-strip::-webkit-scrollbar-thumb {
    background: rgba(255, 255, 255, 0.3);
    border-radius: 3px;
  }

  .gallery-thumbnails-strip::-webkit-scrollbar-thumb:hover {
    background: rgba(255, 255, 255, 0.5);
  }

  .gallery-thumbnail {
    position: relative;
    flex-shrink: 0;
    width: 100px;
    height: 75px;
    border: 2px solid transparent;
    border-radius: 6px;
    overflow: hidden;
    cursor: pointer;
    transition: all 0.2s ease;
    background: none;
    padding: 0;
  }

  .gallery-thumbnail:hover {
    border-color: rgba(255, 255, 255, 0.5);
    transform: scale(1.05);
  }

  .gallery-thumbnail.selected {
    border-color: var(--color-accent);
    box-shadow: 0 0 8px rgba(225, 96, 76, 0.5);
  }

  .gallery-thumbnail img {
    width: 100%;
    height: 100%;
    object-fit: cover;
  }

  .thumbnail-badge {
    position: absolute;
    top: 4px;
    right: 4px;
    background: var(--color-accent);
    color: white;
    font-size: 0.7rem;
    font-weight: 600;
    padding: 2px 6px;
    border-radius: 8px;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.3);
  }

  .empty-gallery {
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: center;
    height: 100%;
    color: rgba(255, 255, 255, 0.6);
    gap: 0.5rem;
  }

  .empty-gallery p {
    margin: 0;
  }

  .empty-gallery .text-muted {
    font-size: 0.9rem;
    opacity: 0.7;
  }

  /* Main Preview for Gallery Images */
  .main-preview {
    width: 100%;
    height: 100%;
    display: flex;
    align-items: center;
    justify-content: center;
    overflow: hidden;
    position: relative;
  }

  .main-preview.zoomed {
    overflow: auto;
  }

  .main-preview img {
    max-width: 100%;
    max-height: 100%;
    object-fit: contain;
    transition: transform 0.1s ease-out;
    transform-origin: center center;
  }

  /* Gallery Controls */
  .gallery-controls {
    background-color: rgba(0, 0, 0, 0.3);
    border-radius: var(--radius-md);
    padding: var(--spacing-md);
    margin-top: var(--spacing-md);
  }

  .gallery-nav {
    position: absolute;
    top: 50%;
    transform: translateY(-50%);
    width: 40px;
    height: 40px;
    background-color: rgba(0, 0, 0, 0.7);
    color: var(--color-white);
    border: 1px solid rgba(255, 255, 255, 0.3);
    border-radius: 50%;
    font-size: 2rem;
    display: flex;
    align-items: center;
    justify-content: center;
    cursor: pointer;
    transition: all var(--transition-fast);
    z-index: 10;
  }

  .gallery-nav:hover:not(:disabled) {
    background-color: var(--color-accent);
    border-color: var(--color-accent);
  }

  .gallery-nav:disabled {
    opacity: 0.3;
    cursor: not-allowed;
  }

  .gallery-nav-prev {
    left: 10px;
  }

  .gallery-nav-next {
    right: 10px;
  }

  .gallery-thumbnails {
    display: flex;
    gap: var(--spacing-sm);
    overflow-x: auto;
    padding: var(--spacing-sm) 0;
    margin-bottom: var(--spacing-sm);
  }

  .gallery-thumbnail {
    flex-shrink: 0;
    width: 80px;
    height: 60px;
    border-radius: var(--radius-sm);
    overflow: hidden;
    border: 2px solid transparent;
    cursor: pointer;
    transition: all var(--transition-fast);
    padding: 0;
    background: none;
    position: relative;
  }

  .gallery-thumbnail img {
    width: 100%;
    height: 100%;
    object-fit: cover;
  }

  .thumbnail-timestamp {
    position: absolute;
    bottom: 2px;
    right: 2px;
    background-color: rgba(0, 0, 0, 0.8);
    color: var(--color-white);
    font-size: 10px;
    padding: 2px 4px;
    border-radius: 3px;
    font-weight: 600;
  }

  .gallery-thumbnail:hover {
    border-color: rgba(255, 255, 255, 0.5);
  }

  .gallery-thumbnail.active {
    border-color: var(--color-accent);
    box-shadow: 0 0 8px rgba(225, 96, 76, 0.5);
  }

  .gallery-indicators {
    display: flex;
    justify-content: center;
    gap: var(--spacing-sm);
  }

  .indicator {
    width: 10px;
    height: 10px;
    border-radius: 50%;
    background-color: rgba(255, 255, 255, 0.3);
    border: none;
    cursor: pointer;
    transition: all var(--transition-fast);
    padding: 0;
  }

  .indicator:hover {
    background-color: rgba(255, 255, 255, 0.5);
  }

  .indicator.active {
    background-color: var(--color-accent);
    width: 30px;
    border-radius: 5px;
  }

  .gallery-thumbnails::-webkit-scrollbar {
    height: 6px;
  }

  .gallery-thumbnails::-webkit-scrollbar-track {
    background: rgba(0, 0, 0, 0.2);
    border-radius: var(--radius-sm);
  }

  .gallery-thumbnails::-webkit-scrollbar-thumb {
    background: rgba(255, 255, 255, 0.3);
    border-radius: var(--radius-sm);
  }

  .gallery-thumbnails::-webkit-scrollbar-thumb:hover {
    background: rgba(255, 255, 255, 0.5);
  }

  /* Results Area */
  .results-area {
    background-color: rgba(0, 0, 0, 0.3);
    border-radius: var(--radius-md);
    padding: var(--spacing-md);
    max-height: 250px;
    overflow-y: auto;
  }

  .results-area h3 {
    margin: 0 0 var(--spacing-md) 0;
    font-size: var(--font-size-lg);
    color: var(--color-white);
  }

  .results-list {
    display: flex;
    flex-wrap: wrap;
    gap: var(--spacing-sm);
    margin-bottom: var(--spacing-md);
  }

  .result-badge {
    padding: var(--spacing-xs) var(--spacing-md);
    background-color: var(--color-accent);
    color: var(--color-white);
    border-radius: var(--radius-sm);
    font-size: var(--font-size-sm);
    font-weight: 500;
  }

  .results-summary {
    padding-top: var(--spacing-sm);
    border-top: 1px solid rgba(255, 255, 255, 0.1);
    color: var(--color-text-light);
    font-size: var(--font-size-sm);
  }

  /* Control Sidebar */
  .control-sidebar {
    width: 320px;
    display: flex;
    flex-direction: column;
    gap: var(--spacing-md);
    overflow-y: auto;
    padding-right: var(--spacing-sm);
  }

  .control-section {
    background-color: rgba(0, 0, 0, 0.3);
    border-radius: var(--radius-md);
    padding: var(--spacing-md);
  }

  .control-section h3 {
    margin: 0 0 var(--spacing-md) 0;
    font-size: var(--font-size-lg);
    color: var(--color-white);
    border-bottom: 2px solid rgba(225, 96, 76, 0.3);
    padding-bottom: var(--spacing-sm);
  }

  .form-group {
    margin-bottom: var(--spacing-md);
  }

  .form-group label {
    display: flex;
    flex-direction: column;
    gap: var(--spacing-xs);
  }

  .form-group span {
    color: var(--color-text-light);
    font-size: var(--font-size-sm);
  }

  .form-group input,
  .file-input-label {
    cursor: pointer;
  }

  .file-name {
    margin-top: var(--spacing-xs);
    font-size: 0.9rem;
    color: var(--color-text);
    font-weight: 500;
    line-height: 1.5;
    word-break: break-word;
    background: rgba(255, 255, 255, 0.05);
    padding: 0.75rem;
    border-radius: 8px;
    border: 1px solid rgba(255, 255, 255, 0.1);
  }

  .btn-start {
    width: 100%;
    margin-bottom: var(--spacing-sm);
    font-weight: 600;
  }

  .btn-start-large {
    padding: var(--spacing-lg) var(--spacing-xl);
    font-size: var(--font-size-lg);
    font-weight: 700;
    letter-spacing: 0.5px;
    box-shadow: 0 4px 12px rgba(225, 96, 76, 0.3);
    transition: all var(--transition-base);
  }

  .btn-start-large:hover:not(:disabled) {
    transform: translateY(-2px);
    box-shadow: 0 6px 16px rgba(225, 96, 76, 0.4);
  }

  .btn-start-large:active:not(:disabled) {
    transform: translateY(0);
    box-shadow: 0 2px 8px rgba(225, 96, 76, 0.3);
  }

  .btn-start-large:disabled {
    background-color: #666 !important;
    border-color: #666 !important;
    color: #999 !important;
    cursor: not-allowed !important;
    opacity: 0.5 !important;
    box-shadow: none !important;
  }

  /* Batch Progress Indicator */
  .batch-progress-indicator {
    margin-bottom: var(--spacing-md);
    padding: var(--spacing-md);
    background: rgba(225, 96, 76, 0.1);
    border: 1px solid rgba(225, 96, 76, 0.3);
    border-radius: var(--radius-md);
  }

  .progress-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: var(--spacing-xs);
    color: var(--color-white);
    font-size: var(--font-size-sm);
    font-weight: 600;
  }

  .progress-count {
    color: var(--color-accent);
  }

  .progress-bar {
    width: 100%;
    height: 8px;
    background: rgba(0, 0, 0, 0.3);
    border-radius: var(--radius-full);
    overflow: hidden;
    margin-bottom: var(--spacing-xs);
  }

  .progress-fill {
    height: 100%;
    background: linear-gradient(90deg, var(--color-accent), #f97316);
    border-radius: var(--radius-full);
    transition: width var(--transition-base);
  }

  .progress-file {
    color: var(--color-text-light);
    font-size: var(--font-size-xs);
    margin: 0;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }

  /* Task-Specific Instructions Styles */
  .task-instructions {
    background: rgba(255, 255, 255, 0.05);
    border: 1px solid rgba(255, 255, 255, 0.15);
    border-radius: var(--radius-md);
    padding: var(--spacing-md);
    margin-bottom: var(--spacing-md);
  }

  .task-badge-header {
    margin-bottom: 0;
    display: flex;
    align-items: center;
    justify-content: space-between;
    cursor: pointer;
    user-select: none;
    transition: all 0.2s ease;
  }

  .task-badge-header:hover {
    opacity: 0.8;
  }

  .collapse-toggle {
    background: transparent;
    border: none;
    color: rgba(255, 255, 255, 0.6);
    font-size: 0.75rem;
    cursor: pointer;
    padding: 0.25rem 0.5rem;
    transition: all 0.2s ease;
  }

  .collapse-toggle:hover {
    color: rgba(255, 255, 255, 0.9);
    transform: scale(1.1);
  }

  .tips-content {
    margin-top: var(--spacing-sm);
    animation: slideDown 0.2s ease;
  }

  @keyframes slideDown {
    from {
      opacity: 0;
      transform: translateY(-10px);
    }
    to {
      opacity: 1;
      transform: translateY(0);
    }
  }

  .task-badge {
    display: inline-block;
    padding: 0.4rem 0.8rem;
    border-radius: 6px;
    font-size: 0.875rem;
    font-weight: 600;
    color: white;
  }

  .task-badge.detect {
    background: linear-gradient(135deg, #3b82f6 0%, #2563eb 100%);
  }

  .task-badge.classify {
    background: linear-gradient(135deg, #e1604c 0%, #dc2626 100%);
  }

  .task-badge.segment {
    background: linear-gradient(135deg, #f59e0b 0%, #d97706 100%);
  }

  .task-description {
    color: rgba(255, 255, 255, 0.85);
    font-size: 0.9rem;
    line-height: 1.5;
    margin: var(--spacing-sm) 0;
  }

  .task-guidance {
    margin-top: var(--spacing-sm);
    padding: var(--spacing-sm);
    background: rgba(59, 130, 246, 0.1);
    border-left: 3px solid #3b82f6;
    border-radius: 4px;
  }

  .task-guidance strong {
    display: block;
    color: #93c5fd;
    font-size: 0.875rem;
    margin-bottom: 0.5rem;
  }

  .task-guidance ul {
    margin: 0;
    padding-left: 1.25rem;
    color: rgba(255, 255, 255, 0.75);
    font-size: 0.85rem;
    line-height: 1.6;
  }

  .task-guidance li {
    margin-bottom: 0.25rem;
  }

  .warning-box {
    margin-top: var(--spacing-sm);
    padding: var(--spacing-sm);
    background: rgba(245, 158, 11, 0.15);
    border: 1px solid rgba(245, 158, 11, 0.3);
    border-radius: 6px;
    color: #fcd34d;
    font-size: 0.85rem;
    line-height: 1.5;
  }

  .btn-reset {
    width: 100%;
    margin-bottom: var(--spacing-md);
    font-size: var(--font-size-base);
  }

  .btn-outline {
    width: 100%;
    margin-bottom: var(--spacing-md);
  }

  .model-selector,
  .source-selector,
  .mode-selector {
    margin-bottom: var(--spacing-md);
  }

  .source-selector label {
    display: flex;
    flex-direction: column;
    gap: var(--spacing-xs);
  }

  .model-selector label {
    display: flex;
    flex-direction: column;
    gap: var(--spacing-xs);
  }

  .model-selector span {
    color: var(--color-text-light);
    font-size: var(--font-size-sm);
  }

  .model-selector select {
    width: 100%;
    background-color: var(--color-navy);
    color: var(--color-bg-light1);
    border: 1px solid rgba(255, 255, 255, 0.2);
    padding: var(--spacing-sm);
    border-radius: var(--radius-sm);
    font-weight: 500;
  }

  .model-selector select option {
    background-color: var(--color-navy);
    color: var(--color-bg-light1);
  }

  .source-selector span,
  .source-selector select,
  .source-selector select option,

  /* Smart Settings */
  .class-filter-control {
    margin-bottom: 1rem;
  }

  .class-filter-control div {
    display: block;
    margin-bottom: 0.5rem;
  }

  .class-filter-control span {
    font-weight: 600;
    color: #374151;
  }

  .tags-input-container {
    width: 100%;
    min-height: 42px;
    padding: 0.5rem;
    background: white;
    border: 1px solid #d1d5db;
    border-radius: 6px;
    transition: all 0.2s ease;
  }

  .tags-input-container:focus-within {
    border-color: #e1604c;
    box-shadow: 0 0 0 3px rgba(225, 96, 76, 0.1);
  }

  .tags-list {
    display: flex;
    flex-wrap: wrap;
    gap: 0.5rem;
    align-items: center;
  }

  .tag {
    display: inline-flex;
    align-items: center;
    gap: 0.375rem;
    padding: 0.375rem 0.625rem;
    background: #4b5563;
    color: white;
    border-radius: 4px;
    font-size: 0.875rem;
    font-weight: 500;
    transition: all 0.2s ease;
  }

  .tag-remove {
    display: flex;
    align-items: center;
    justify-content: center;
    width: 18px;
    height: 18px;
    padding: 0;
    background: rgba(255, 255, 255, 0.2);
    border: none;
    border-radius: 50%;
    color: white;
    font-size: 16px;
    font-weight: bold;
    line-height: 1;
    cursor: pointer;
    transition: all 0.2s ease;
  }

  .tag-remove:hover {
    background: rgba(255, 255, 255, 0.3);
  }

  .tag-remove:disabled {
    cursor: not-allowed;
    opacity: 0.5;
  }

  .tag-input {
    flex: 1;
    min-width: 120px;
    padding: 0.25rem;
    border: none;
    outline: none;
    font-size: 0.875rem;
    background: transparent;
  }

  .tag-input::placeholder {
    color: #9ca3af;
  }

  .tag-input:disabled {
    cursor: not-allowed;
    background: #f3f4f6;
  }

  .segment-controls {
    margin-bottom: var(--spacing-md);
    padding: var(--spacing-md);
    background: rgba(225, 96, 76, 0.1);
    border: 1px solid rgba(225, 96, 76, 0.3);
    border-radius: var(--radius-md);
  }

  .segment-controls div {
    display: block;
    margin-bottom: var(--spacing-sm);
    color: var(--color-white);
    font-weight: 600;
    font-size: var(--font-size-sm);
  }

  .prompt-mode-selector {
    display: grid;
    grid-template-columns: repeat(4, 1fr);
    gap: var(--spacing-xs);
    margin-bottom: var(--spacing-sm);
  }

  .prompt-mode-btn {
    padding: var(--spacing-sm) var(--spacing-xs);
    background: rgba(255, 255, 255, 0.1);
    border: 2px solid rgba(255, 255, 255, 0.2);
    border-radius: var(--radius-sm);
    color: var(--color-white);
    font-size: var(--font-size-xs);
    font-weight: 600;
    cursor: pointer;
    transition: all var(--transition-base);
  }

  .prompt-mode-btn:hover:not(:disabled) {
    background: rgba(255, 255, 255, 0.2);
    border-color: rgba(255, 255, 255, 0.4);
    transform: translateY(-1px);
  }

  .prompt-mode-btn.active {
    background: var(--color-accent);
    border-color: var(--color-accent);
    box-shadow: 0 2px 8px rgba(225, 96, 76, 0.3);
  }

  .prompt-mode-btn:disabled {
    opacity: 0.5;
    cursor: not-allowed;
  }

  .prompt-hint {
    margin-bottom: var(--spacing-sm);
    padding: var(--spacing-sm);
    background: rgba(255, 255, 255, 0.05);
    border-left: 3px solid var(--color-accent);
    border-radius: var(--radius-sm);
    font-size: var(--font-size-xs);
    color: var(--color-text-light);
  }

  .text-prompt-input {
    display: flex;
    gap: var(--spacing-sm);
    margin-bottom: var(--spacing-sm);
  }

  .text-prompt-input input {
    flex: 1;
    padding: var(--spacing-sm);
    background: rgba(255, 255, 255, 0.1);
    border: 1px solid rgba(255, 255, 255, 0.2);
    border-radius: var(--radius-sm);
    color: var(--color-white);
    font-size: var(--font-size-sm);
  }

  .text-prompt-input input::placeholder {
    color: rgba(255, 255, 255, 0.4);
  }

  .text-prompt-input input:focus {
    outline: none;
    border-color: var(--color-accent);
    box-shadow: 0 0 0 2px rgba(225, 96, 76, 0.2);
  }

  .prompts-list {
    margin-top: var(--spacing-sm);
    padding: var(--spacing-sm);
    background: rgba(0, 0, 0, 0.2);
    border-radius: var(--radius-sm);
  }

  .prompts-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: var(--spacing-sm);
    padding-bottom: var(--spacing-xs);
    border-bottom: 1px solid rgba(255, 255, 255, 0.1);
  }

  .prompts-header span {
    color: var(--color-white);
    font-size: var(--font-size-xs);
    font-weight: 600;
  }

  .btn-clear {
    padding: var(--spacing-xs) var(--spacing-sm);
    background: rgba(239, 68, 68, 0.2);
    border: 1px solid rgba(239, 68, 68, 0.4);
    border-radius: var(--radius-sm);
    color: #fca5a5;
    font-size: var(--font-size-xs);
    font-weight: 600;
    cursor: pointer;
    transition: all var(--transition-base);
  }

  .btn-clear:hover:not(:disabled) {
    background: rgba(239, 68, 68, 0.3);
    border-color: rgba(239, 68, 68, 0.6);
  }

  .btn-clear:disabled {
    opacity: 0.5;
    cursor: not-allowed;
  }

  .prompts-items {
    display: flex;
    flex-direction: column;
    gap: var(--spacing-xs);
  }

  .prompt-item {
    display: flex;
    align-items: center;
    gap: var(--spacing-sm);
    padding: var(--spacing-sm);
    background: rgba(255, 255, 255, 0.05);
    border-radius: var(--radius-sm);
    transition: all var(--transition-base);
  }

  .prompt-item:hover {
    background: rgba(255, 255, 255, 0.1);
  }

  .prompt-icon {
    font-size: var(--font-size-base);
  }

  .prompt-label {
    flex: 1;
    color: var(--color-white);
    font-size: var(--font-size-xs);
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }

  .btn-remove {
    width: 20px;
    height: 20px;
    padding: 0;
    background: rgba(239, 68, 68, 0.2);
    border: none;
    border-radius: 50%;
    color: #fca5a5;
    font-size: 16px;
    font-weight: bold;
    line-height: 1;
    cursor: pointer;
    transition: all var(--transition-base);
  }

  .btn-remove:hover:not(:disabled) {
    background: rgba(239, 68, 68, 0.4);
  }

  .btn-remove:disabled {
    opacity: 0.5;
    cursor: not-allowed;
  }

  .confidence-control {
    margin-bottom: var(--spacing-md);
  }

  .confidence-control label {
    display: flex;
    flex-direction: column;
    gap: var(--spacing-xs);
  }

  .confidence-control span {
    color: var(--color-white);
    font-size: var(--font-size-sm);
    font-weight: 500;
  }

  .confidence-control input[type="range"] {
    width: 100%;
    height: 6px;
    background: rgba(255, 255, 255, 0.2);
    border-radius: var(--radius-sm);
    outline: none;
    appearance: none;
    -webkit-appearance: none;
  }

  .confidence-control input[type="range"]::-webkit-slider-thumb {
    -webkit-appearance: none;
    appearance: none;
    width: 18px;
    height: 18px;
    background: var(--color-accent);
    border-radius: 50%;
    cursor: pointer;
  }

  .confidence-control input[type="range"]::-moz-range-thumb {
    width: 18px;
    height: 18px;
    background: var(--color-accent);
    border-radius: 50%;
    cursor: pointer;
    border: none;
  }

  /* Skip Frames Control */
  .skip-frames-control {
    margin-bottom: var(--spacing-md);
  }

  .skip-frames-control label {
    display: flex;
    flex-direction: column;
    gap: var(--spacing-xs);
  }

  .skip-frames-control span {
    color: var(--color-white);
    font-size: var(--font-size-sm);
    font-weight: 500;
  }

  .skip-frames-control input[type="range"] {
    width: 100%;
    height: 6px;
    background: rgba(255, 255, 255, 0.2);
    border-radius: var(--radius-sm);
    outline: none;
    appearance: none;
    -webkit-appearance: none;
  }

  .skip-frames-control input[type="range"]::-webkit-slider-thumb {
    -webkit-appearance: none;
    appearance: none;
    width: 18px;
    height: 18px;
    background: var(--color-accent);
    border-radius: 50%;
    cursor: pointer;
  }

  .skip-frames-control input[type="range"]::-moz-range-thumb {
    width: 18px;
    height: 18px;
    background: var(--color-accent);
    border-radius: 50%;
    cursor: pointer;
    border: none;
  }

  .help-text {
    margin-top: var(--spacing-xs);
    font-size: var(--font-size-xs);
    color: var(--color-text-light);
  }

  .warning-badge {
    display: inline-block;
    padding: 4px 8px;
    background: var(--color-warning);
    color: white;
    border-radius: var(--radius-sm);
    font-weight: 600;
    font-size: 0.75rem;
  }

  .info-text {
    margin: 0;
    padding: 0;
  }

  .zoom-control {
    margin-bottom: var(--spacing-md);
    padding-top: var(--spacing-md);
    border-top: 1px solid rgba(255, 255, 255, 0.1);
  }

  .zoom-control label {
    display: flex;
    flex-direction: column;
    gap: var(--spacing-xs);
  }

  .zoom-control span {
    color: var(--color-white);
    font-size: var(--font-size-sm);
    font-weight: 500;
  }

  .zoom-control input[type="range"] {
    width: 100%;
    height: 6px;
    background: rgba(255, 255, 255, 0.2);
    border-radius: var(--radius-sm);
    outline: none;
    appearance: none;
    -webkit-appearance: none;
  }

  .zoom-control input[type="range"]::-webkit-slider-thumb {
    -webkit-appearance: none;
    appearance: none;
    width: 18px;
    height: 18px;
    background: var(--color-accent);
    border-radius: 50%;
    cursor: pointer;
  }

  .zoom-control input[type="range"]::-moz-range-thumb {
    width: 18px;
    height: 18px;
    background: var(--color-accent);
    border-radius: 50%;
    cursor: pointer;
    border: none;
  }

  .zoom-control button {
    margin-top: var(--spacing-sm);
  }

  .class-filters {
    margin-top: var(--spacing-md);
  }

  .filter-label {
    display: block;
    margin-bottom: var(--spacing-sm);
    color: var(--color-text-light);
    font-size: var(--font-size-sm);
  }

  .filter-buttons {
    display: flex;
    flex-wrap: wrap;
    gap: var(--spacing-xs);
  }

  .filter-btn {
    padding: var(--spacing-xs) var(--spacing-sm);
    background-color: rgba(255, 255, 255, 0.1);
    color: var(--color-white);
    border: 1px solid rgba(255, 255, 255, 0.2);
    border-radius: var(--radius-sm);
    font-size: var(--font-size-sm);
    cursor: pointer;
    transition: all var(--transition-fast);
  }

  .filter-btn:hover {
    background-color: rgba(255, 255, 255, 0.2);
  }

  .filter-btn.active {
    background-color: var(--color-accent);
    border-color: var(--color-accent);
  }

  /* Frame Stats */
  .frame-stats {
    display: flex;
    flex-direction: column;
    gap: var(--spacing-sm);
  }

  .stat-row {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: var(--spacing-xs) 0;
  }

  .stat-label {
    color: var(--color-text-light);
    font-size: var(--font-size-sm);
  }

  .stat-value {
    color: var(--color-white);
    font-weight: 600;
    font-size: var(--font-size-base);
  }

  /* Job Status */
  .job-status {
    display: flex;
    flex-direction: column;
    gap: var(--spacing-sm);
  }

  .status-badge {
    padding: var(--spacing-xs) var(--spacing-sm);
    border-radius: var(--radius-sm);
    text-align: center;
    font-size: var(--font-size-sm);
    font-weight: 600;
  }

  .progress-bar {
    width: 100%;
    height: 8px;
    background-color: rgba(255, 255, 255, 0.2);
    border-radius: var(--radius-sm);
    overflow: hidden;
  }

  .progress-fill {
    height: 100%;
    background: linear-gradient(
      90deg,
      var(--color-accent),
      var(--color-status-success)
    );
    transition: width var(--transition-base);
  }

  .progress-text {
    text-align: center;
    color: var(--color-white);
    font-size: var(--font-size-sm);
    margin: 0;
  }

  /* Scrollbar Styling */
  .control-sidebar::-webkit-scrollbar,
  .results-area::-webkit-scrollbar {
    width: 8px;
  }

  .control-sidebar::-webkit-scrollbar-track,
  .results-area::-webkit-scrollbar-track {
    background: rgba(0, 0, 0, 0.2);
    border-radius: var(--radius-sm);
  }

  .control-sidebar::-webkit-scrollbar-thumb,
  .results-area::-webkit-scrollbar-thumb {
    background: rgba(255, 255, 255, 0.3);
    border-radius: var(--radius-sm);
  }

  .control-sidebar::-webkit-scrollbar-thumb:hover,
  .results-area::-webkit-scrollbar-thumb:hover {
    background: rgba(255, 255, 255, 0.5);
  }

  /* Camera Flash Overlay */
  .camera-flash-overlay {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background: white;
    pointer-events: none;
    z-index: 1000;
    animation: flash 0.3s ease-out;
  }

  @keyframes flash {
    0% {
      opacity: 0.8;
    }
    100% {
      opacity: 0;
    }
  }

  /* Capture Button */
  .btn-capture {
    width: 100%;
    margin-bottom: 0.5rem;
    background: var(--color-accent);
    color: white;
    font-weight: 600;
    padding: 0.875rem 1.5rem;
    border: none;
    border-radius: var(--radius-md);
    cursor: pointer;
    transition: all var(--transition-base);
  }

  .btn-capture:hover:not(:disabled) {
    background: #d55540;
    transform: translateY(-2px);
    box-shadow: 0 4px 12px rgba(225, 96, 76, 0.3);
  }

  .btn-capture:active:not(:disabled) {
    transform: translateY(0);
  }

  .btn-capture:disabled {
    opacity: 0.5;
    cursor: not-allowed;
  }

  /* Capture Counter */
  .capture-counter {
    text-align: center;
    font-size: 0.875rem;
    color: var(--color-text-secondary);
    margin: 0.5rem 0 1rem 0;
  }

  .capture-counter strong {
    color: var(--color-accent);
    font-size: 1rem;
  }

  /* Clear Gallery Button */
  .btn-clear-gallery {
    position: absolute;
    top: 1rem;
    right: 1rem;
    z-index: 10;
    display: flex;
    align-items: center;
    gap: 0.5rem;
    padding: 0.75rem 1.25rem;
    font-size: 0.9rem;
    font-weight: 600;
    background: rgba(255, 59, 48, 0.9);
    color: white;
    border: 2px solid rgba(255, 59, 48, 1);
    border-radius: 8px;
    cursor: pointer;
    transition: all 0.2s ease;
    box-shadow:
      0 4px 12px rgba(255, 59, 48, 0.3),
      0 0 0 0 rgba(255, 59, 48, 0.4);
    backdrop-filter: blur(10px);
  }

  .btn-clear-gallery:hover {
    background: rgba(255, 59, 48, 1);
    transform: translateY(-2px);
    box-shadow:
      0 6px 16px rgba(255, 59, 48, 0.4),
      0 0 0 4px rgba(255, 59, 48, 0.2);
  }

  .btn-clear-gallery:active {
    transform: translateY(0);
    box-shadow: 0 2px 8px rgba(255, 59, 48, 0.3);
  }

  .clear-icon {
    font-size: 1.1rem;
    line-height: 1;
  }

  .clear-text {
    font-weight: 600;
  }

  .clear-count {
    opacity: 0.9;
    font-size: 0.85rem;
    font-weight: 500;
  }

  /* Video Session Banner */
  .session-banner {
    display: flex;
    align-items: center;
    justify-content: space-between;
    background: linear-gradient(
      135deg,
      rgba(225, 96, 76, 0.15),
      rgba(29, 47, 67, 0.3)
    );
    border: 2px solid rgba(225, 96, 76, 0.4);
    border-radius: 12px;
    padding: 1rem 1.25rem;
    margin-bottom: 1rem;
    backdrop-filter: blur(10px);
  }

  .session-info {
    display: flex;
    align-items: center;
    gap: 1rem;
    flex: 1;
  }

  .session-icon {
    font-size: 1.5rem;
  }

  .session-details {
    display: flex;
    flex-direction: column;
    gap: 0.25rem;
  }

  .session-details strong {
    color: rgba(255, 255, 255, 0.9);
    font-size: 0.875rem;
    font-weight: 600;
  }

  .session-filename {
    color: var(--color-accent);
    font-size: 0.95rem;
    font-weight: 500;
  }

  .session-stats {
    display: flex;
    align-items: center;
    gap: 1rem;
    margin-left: auto;
  }

  .capture-count {
    color: rgba(255, 255, 255, 0.7);
    font-size: 0.875rem;
  }

  /* Inactivity Modal */
  .modal-overlay {
    position: fixed;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background: rgba(0, 0, 0, 0.7);
    backdrop-filter: blur(5px);
    display: flex;
    align-items: center;
    justify-content: center;
    z-index: 9999;
    animation: fadeIn 0.2s ease-out;
  }

  @keyframes fadeIn {
    from {
      opacity: 0;
    }
    to {
      opacity: 1;
    }
  }

  .modal-content {
    background: var(--color-navy);
    border-radius: 16px;
    padding: 2rem;
    max-width: 500px;
    width: 90%;
    box-shadow: 0 8px 32px rgba(0, 0, 0, 0.5);
    border: 1px solid rgba(255, 255, 255, 0.1);
    animation: slideUp 0.3s ease-out;
  }

  @keyframes slideUp {
    from {
      transform: translateY(20px);
      opacity: 0;
    }
    to {
      transform: translateY(0);
      opacity: 1;
    }
  }

  .inactivity-modal {
    text-align: center;
  }

  .modal-icon {
    font-size: 3rem;
    margin-bottom: 1rem;
  }

  .inactivity-modal h2 {
    color: var(--color-white);
    font-size: 1.5rem;
    margin-bottom: 0.75rem;
  }

  .inactivity-modal p {
    color: rgba(255, 255, 255, 0.8);
    font-size: 1rem;
    margin-bottom: 0.5rem;
  }

  .modal-subtitle {
    font-size: 0.9rem;
    color: rgba(255, 255, 255, 0.6);
    margin-bottom: 1.5rem;
  }

  .modal-actions {
    display: flex;
    gap: 1rem;
    justify-content: center;
    margin-top: 1.5rem;
  }

  .modal-actions .btn {
    padding: 0.75rem 1.5rem;
    font-size: 1rem;
  }

  /* Responsive adjustments */
  @media (max-width: 1200px) {
    .control-sidebar {
      width: 280px;
    }
  }
</style>
